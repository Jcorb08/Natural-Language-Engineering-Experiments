{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 3: Question 3\n",
    "\n",
    "For this assessment, you are expected to complete and submit 4 notebook files.  There is 1 notebook file for each question (to speed up load times).  This is notebook 3 out of 4.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateno=198735 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is /Users/Joe/Documents/Python Scripts/resources/resources\n"
     ]
    }
   ],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/Joe/Documents/Python Scripts/resources/resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import spacy\n",
    "#nlp=spacy.load('en')\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import gutenberg\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Named Entity Recognition and Linking (25 marks)\n",
    "\n",
    "The code below will run the SpaCy system on the text from Persuasion by Jane Austen.  `mysample` contains a 50% sample which is unique to your candidate number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do NOT change the code in this cell.\n",
    "\n",
    "#preparing corpus\n",
    "\n",
    "def clean_text(astring):\n",
    "    #replace newlines with space\n",
    "    newstring=re.sub(\"\\n\",\" \",astring)\n",
    "    #remove title and chapter headings\n",
    "    newstring=re.sub(\"\\[[^\\]]*\\]\",\" \",newstring)\n",
    "    newstring=re.sub(\"VOLUME \\S+\",\" \",newstring)\n",
    "    newstring=re.sub(\"CHAPTER \\S+\",\" \",newstring)\n",
    "    newstring=re.sub(\"\\s\\s+\",\" \",newstring)\n",
    "    #return re.sub(\"([^\\.|^ ])  +\",r\"\\1 .  \",newstring).lstrip().rstrip()\n",
    "    return newstring.lstrip().rstrip()\n",
    "\n",
    "\n",
    "def get_sample(sentslist,seed=candidateno):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(sentslist)\n",
    "    testsize=int(len(sentslist)/2)\n",
    "    return sentslist[testsize:]\n",
    "    \n",
    "persuasion=clean_text(gutenberg.raw('austen-persuasion.txt'))\n",
    "nlp_persuasion=list(nlp(persuasion).sents)\n",
    "\n",
    "mysample=get_sample(nlp_persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mysample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) **Write code** and **extract**:\n",
    "* the 30 most common strings referring to PEOPLE in `mysample`.\n",
    "* the 30 most common strings referring to PLACES in `mysample`.\n",
    "\n",
    "\\[6 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_lists(sents):\n",
    "    tokens=[]\n",
    "    pos=[]\n",
    "    ner=[]\n",
    "    for sent in sents:\n",
    "        for t in sent:\n",
    "            tokens.append(t.text)\n",
    "            pos.append(t.pos_)\n",
    "            ner.append(t.ent_type_)\n",
    "    return tokens,pos,ner\n",
    "\n",
    "def extract_entities(tokenlist,taglist,tagtype):\n",
    "    \n",
    "    entities={}\n",
    "    inentity=False\n",
    "    for i,(token,tag) in enumerate(zip(tokenlist,taglist)):\n",
    "        if tag==tagtype:\n",
    "            if inentity:\n",
    "                entity+=\" \"+token\n",
    "            else:\n",
    "                entity=token\n",
    "                inentity=True\n",
    "        elif inentity:\n",
    "            entities[entity]=entities.get(entity,0)+1\n",
    "            inentity=False\n",
    "    return entities  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most common strings referring to PEOPLE:\n",
      "[('Anne', 212), ('Elliot', 113), ('Mary', 73), ('Captain Wentworth', 62), ('Charles', 61), ('Walter', 59), ('Lady Russell', 50), ('Elizabeth', 46), ('Wentworth', 26), ('Louisa', 24), ('Musgrove', 19), ('Mrs Musgrove', 18), ('Harville', 17), ('Mrs Clay', 17), ('Captain Benwick', 16), ('Smith', 15), ('Charles Hayter', 14), ('Wallis', 14), ('Lady Dalrymple', 14), ('Mrs Smith', 13), ('Musgroves', 13), (\"Lady Russell 's\", 12), ('Benwick', 12), ('Captain Harville', 9), ('Mr Shepherd', 8), ('Charles Musgrove', 7), ('Lyme', 7), ('Anne Elliot', 7), ('Mrs Croft', 7), (\"Captain Wentworth 's\", 7)]\n",
      "30 Most common strings referring to PLACES:\n",
      "[('Uppercross', 27), ('Kellynch', 11), ('Camden Place', 10), ('Lyme', 7), ('Monkford', 6), ('the Great House', 5), ('the West Indies', 4), ('Plymouth', 3), ('London', 3), ('the Octagon Room', 3), ('Cape', 2), ('the Concert Room', 2), ('Mrs Musgrove', 2), ('Uppercross Cottage', 2), (\"Captain Wentworth 's\", 2), ('Rivers Street', 2), ('Mackenzie', 1), ('the Mr Wentworth', 1), ('Mediterranean', 1), ('Mrs Wallis', 1), ('Atlantic', 1), ('the East Indies', 1), ('South Park', 1), ('Gloucester', 1), ('England', 1), ('Harville', 1), ('Bath', 1), ('Hayters', 1), ('yestermorn', 1), ('Bahama', 1)]\n"
     ]
    }
   ],
   "source": [
    "tokens,tag,refer = make_tag_lists(mysample)\n",
    "#print(tag_list)\n",
    "people=extract_entities(tokens,refer,\"PERSON\")\n",
    "top_people=sorted(people.items(),key=operator.itemgetter(1),reverse=True)[:30]\n",
    "print(\"30 Most common strings referring to PEOPLE:\")\n",
    "print(top_people)\n",
    "def add_to_dict(dict1,dict2):\n",
    "    for key in dict2.keys():\n",
    "        if dict1.get(key,0) != 0:\n",
    "            dict1[key] = dict1[key] + dict2[key]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]\n",
    "    return dict1\n",
    "places=extract_entities(tokens,refer,\"LOC\")\n",
    "temp = extract_entities(tokens,refer,\"GPE\")\n",
    "places = add_to_dict(places,temp)\n",
    "temp2 = extract_entities(tokens,refer,\"FAC\")\n",
    "places = add_to_dict(places,temp2)\n",
    "top_places=sorted(places.items(),key=operator.itemgetter(1),reverse=True)[:30]\n",
    "print(\"30 Most common strings referring to PLACES:\")\n",
    "print(top_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Making reference to specific examples from the text in `mysample`, **discuss** the different types of errors made by the named entity recogniser. \\[6 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities that are given a type may have multiple different types. For example: The entity *'Mrs. Wallis'* can be a person or a place in the context of the text in `mysample`. This could be solved by looking at the surrounding text in a context window in order to gain an idea as to which the entity is referring to in the sentence. There maybe variation of the same entity that is recognised as a different type by the named entity recogniser. For example: *'Captain Benwick'* and *'Benwick'* are variation of a person. This can be solved by having a database that contains named entities and their variations. An entity maybe of the same type, for example: there are multiple places in the world that could be called *'Rivers Street'*. This can therefore lead to ambiguity and an understanding about which *'Rivers street'* it is referring to could help to correctly establish the difference between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) **Design** and **implement** a system to track the locations of characters throughout the story.  For a given PERSON named entity, your system should return a list of time-ordered LOCATIONS for that character.  Test your system using the complete text of \"Persuasion\" (**not** `mysample`) for at least 3 major characters.   \\[13 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system will look through the text and relate person to location in the sentence that it is currently in, using the punctuation and entity tags given by the nlp function, denoted by punctuation that stops a sentence *'.','!','?'*. The input being the person we are looking for and the tokens that we are looking through, output is the person and a time ordered list i.e. appended to the end of the list of the locations that person has been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anne': ['London', 'Kellynch', 'Uppercross Cottage', 'Uppercross Cottage', 'Uppercross Cottage', 'Kellynch', \"Mrs Musgrove 's\", 'England', \"Captain Wentworth 's\", 'Uppercross', 'Uppercross', 'the Mansion House', 'Mr', 'Uppercross', 'Camden Place', 'Kellynch Hall', 'Uppercross', 'Uppercross', 'Camden Place', 'Camden Place', 'Westgate Buildings', 'Westgate Buildings', 'England', 'Ireland', 'Kellynch', 'Uppercross', 'Lyme', 'Camden Place', 'Milsom Street', 'Camden Place', 'Pulteney Street', 'the Mr Wentworth', 'Monkford', 'Camden', 'Camden Place', 'Uppercross', 'Mrs Musgrove', 'Rivers Street', 'Uppercross Hall', \"Captain Wentworth 's\"]}\n",
      "{'Elliot': ['Kellynch Hall', 'Somersetshire', 'Esq', 'Esq .', 'London', 'Sidmouth', 'Uppercross', 'Camden Place', 'Camden Place', 'Kellynch', 'the Dowager Viscountess', 'Westgate Buildings', 'England', 'Ireland', 'Kellynch', 'Rivers Street']}\n",
      "{'Mary': ['Kellynch', 'Uppercross Cottage', 'Uppercross Cottage', 'the Great House', 'the Great House', 'the Great House', 'Uppercross', 'Uppercross', 'Camden Place', 'the Pump Room']}\n"
     ]
    }
   ],
   "source": [
    "def person_locations(tokens, person):\n",
    "    locations = []\n",
    "    location_change = 0\n",
    "    current_location = ''\n",
    "    current_entity = ''\n",
    "    for token in tokens:\n",
    "        if token.ent_type_ in ('LOC','FAC','GPE') and location_change == 1:\n",
    "            if token.ent_type_ == current_entity:\n",
    "                current_location += ' ' + token.text\n",
    "                current_entity = token.ent_type_\n",
    "            else:\n",
    "                if current_location != '':\n",
    "                    locations.append(current_location[1:])\n",
    "                current_location = ' ' + token.text\n",
    "                current_entity = token.ent_type_\n",
    "        else:\n",
    "            if token.text == person:\n",
    "                location_change = 1\n",
    "            elif token.text in ('.','!','?'):\n",
    "                location_change = 0          \n",
    "            if current_location != '':\n",
    "                locations.append(current_location[1:])    \n",
    "            current_location = ''\n",
    "            current_entity = ''            \n",
    "    return {person : locations}\n",
    "\n",
    "text = nlp(persuasion)\n",
    "print(person_locations(text,'Anne'))\n",
    "print(person_locations(text,'Elliot'))\n",
    "print(person_locations(text,'Mary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission length is 238\n"
     ]
    }
   ],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "##Running it before providing any answers shows that the questions have a word count of 1202\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "\n",
    "filepath=\"a3_3.ipynb\"\n",
    "question_count=211\n",
    "\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
