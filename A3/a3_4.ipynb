{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 3: Question 4\n",
    "\n",
    "For this assessment, you are expected to complete and submit 4 notebook files.  There is 1 notebook file for each question (to speed up load times).  This is notebook 4 out of 4.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Question Answering (25 marks)\n",
    "Imagine that you have access to a database containing English-language newspaper and magazine articles for the last 50 years. **Describe in detail** how you would go about building a question-answering system to answer questions about historical events including:\n",
    "* Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?\n",
    "* In which years has the football World Cup been held in France?\n",
    "* Where did Princess Diana die?\n",
    "\n",
    "You are not expected to write any code for this question.  You should, however, describe the challenges and the strategies your system will employ to attempt to overcome them.\n",
    "\n",
    "As no code is expected, you can, if you wish, submit a pdf rather than a notebook file for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A question answering system outputs simple facts expressed in short texts to answer a question. Answers are either right or wrong. Question Answering is concerned with relationships between named entities such as the relations between the question *'Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?'* are: leader, National Union of miners, miners' strikes, 1980s. From these named entities the answer can be established by finding the relationships between them. Question Answering has two different paradigms; Information Retrieval, and Knowledge-based. Information Retrieval (IR-based), uses retrieval techniques to find relevant documents and passages. It uses reading comprehension algorithms to draw an answer from relevant spans of text. Knowledge-based builds a semantic or logical representation of the query, and uses logical meaning representations to query a database of facts. This paradigm is not useful or appropriate in this case, as the database is not a database of facts, it is a database of relevant documents. \n",
    "\n",
    "The Information Retrieval based paradigm must structure the data that it retrieves the answers from, such as the database of newspaper and magazine articles from the last 50 years. Using ranked retrieval because the most relevant answer to our query is required. Boonlean retrieval has no ranking and no partial matches and uses a boolean combination of keywords and connects them by logical operators - not a suitable retrieval for this question answering system. By searching for a query as explained above these queries have terms which help to correctly identify the answer. In order to find these terms quickly; initially, normalise case and number; remove stopwords and lemmenise the documents. This is done to reduce term vocabulary which improves efficiency and performance. Secondly, we index the documents according to their query terms. There are multiple ways to employ in order to rank them effectively, either by Wordnet synsets to find similar words and match them or use named entity recognition to find tags relating to our query. For example, a location if we were trying to find *'Where did Princess Diana die?'*. \n",
    "\n",
    "The retrieval from the indexed documents works by looking up posting lists for each term in the query in the inverted index. It starts with one posting list and merges the documents by intersection with other lists. It then returns documents after all posting lists have been considered. Therefore only the same indexes from each document and posting list end up in the return statement. With a list of potential documents, they are ranked again, ordering them by relevance. If the document has less common query terms then this is more relevant than query terms that occur more frequently in a document. The relevance is calculated using tf-idf for each word in a document or query. Once these are calculated they are scored  by either summing all the values together for a document or getting the product of the tf-idf scores. The former is for OR queries where it could be this answer or a multitude of different answers which are all variations of one another. The latter is more appropriate for AND queries where all the words of the query need to be in the document. If one word is not related at all the score is 0, and deemed not relevant. The answer therefore is the document or documents with the highest relevance scores, and this question answering system will use the adding calculation, as the answer to *'In which years has the football world cup been held in France?'*, maybe a multitude of years, and due to these being articles they are unlikely to be in the same article so multiple will need to be picked in order to obtain the correct answer to the query.\n",
    "\n",
    "One challenge to this system is the accuracy of the system. This is overcome by using the ranking and tf-idf ratings in order to make sure the words are the most relevant to the query inputted. A further challenge is the efficiency of the system; finding what should be found reasonably quickly. The use of ranking in this system allows only the relevant documents to be searched in the first place increasing efficiency. A final challenge is the relevance ranking; presenting users with information in a manageable way. The queries inputted into our system may have many highly relevant content to multiple documents, these documents therefore will be shown first, i.e. by order of the relevance calculated by this system. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission length is 743\n"
     ]
    }
   ],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "##Running it before providing any answers shows that the questions have a word count of 1202\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "\n",
    "filepath=\"a3_4.ipynb\"\n",
    "question_count=174\n",
    "\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
