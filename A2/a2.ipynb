{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 2\n",
    "\n",
    "For this assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about these coursework questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateno=198735 #this MUST be updated to your candidate number so that you get a unique data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is /Users/Joe/Documents/Python Scripts/resources/resources\n"
     ]
    }
   ],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/Joe/Documents/Python Scripts/resources/resources')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "import random\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Document Similarity (25 marks)\n",
    "The objective of this question is to investigate whether incorporating lexical knowledge from WordNet might improve document similarity methods.  For example, knowing that both *tiger* and *leopard* are hyponyms of *big_cat* should increase the similarity between a document mentioning a *tiger* and a document mentioning a *leopard*.\n",
    "\n",
    "The code below will generate two document collections, both in bag-of-words format, one from the Medline Corpus and one from the Wall Street Journal corpus.\n",
    "\n",
    "In this question, there are marks available for the quality of your code and the quality of your explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import MedlineCorpusReader\n",
    "from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalise(tokenlist):\n",
    "    tokenlist=[token.lower() for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if token.isdigit() else token for token in tokenlist]\n",
    "    tokenlist=[\"Nth\" if (token.endswith((\"nd\",\"st\",\"th\")) and token[:-2].isdigit()) else token for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if re.search(\"^[+-]?[0-9]+\\.[0-9]\",token) else token for token in tokenlist]\n",
    "    return tokenlist\n",
    "\n",
    "def filter_stopwords(tokenlist):\n",
    "    stop = stopwords.words('english')\n",
    "    return [w for w in tokenlist if w.isalpha() and w not in stop]\n",
    "\n",
    "def stem(tokenlist):\n",
    "    st=WordNetLemmatizer()\n",
    "    return [st.lemmatize(token) for token in tokenlist]\n",
    "\n",
    "   \n",
    "def make_bow(somestring):\n",
    "    rep=word_tokenize(somestring)  #step 1\n",
    "    rep=normalise(rep)   #step 2\n",
    "    rep=stem(rep)   #step 3\n",
    "    rep=filter_stopwords(rep)  #step 4\n",
    "    dict_rep={}\n",
    "    for token in rep:\n",
    "        dict_rep[token]=dict_rep.get(token,0)+1  #step 5\n",
    "    return(dict_rep)\n",
    "\n",
    "wsj=WSJCorpusReader()\n",
    "medline=MedlineCorpusReader()\n",
    "\n",
    "collectionsize=50\n",
    "collections={\"wsj\":[],\"medline\":[]}\n",
    "\n",
    "for key in collections.keys():\n",
    "    if key==\"wsj\":\n",
    "        generator=wsj.raw()\n",
    "    else:\n",
    "        generator=medline.raw()\n",
    "    while len(collections[key])<collectionsize:\n",
    "        collections[key].append(next(generator))\n",
    "\n",
    "bow_collections={key:[make_bow(doc) for doc in collection] for key,collection in collections.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a). For each step in the `make_bow()` function, **explain** what it does and why it is applicable when creating document representations for document similarity methods. \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1 - The variable rep is assigned to the output of the function \"word_tokenize\". This takes in a string, in this case the variable \"somestring\", and splits the string up into words. It does this by creating a list, and adding a substring of the string when it hits whitespace; this would be a word and therefore a token. It also makes a token for punctuation found in the string as well such as \",\" and \".\" - these are substrings by themselves. The output is the list of all the tokens defined by the function. This is important as it allows us to identify similar tokens, use the tokens as keys and iterate easily through them.\n",
    "* Step 2 - The variable \"rep\", a list of tokens, is a parameter and assigned to the output of the function \"normalise\". The list has 4 actions taken upon it which each reassign the tokenlist, after which a list of tokens is returned in this manner:\n",
    "    * The first action is that for each token in the list the case is set to lowercase. This allows us to ensure that \"The\" is the same as \"the\", as this would otherwise produce false results.\n",
    "    * The second action is a digit check, and if the current token is found to be a digit it replaces it with the string \"NUM\". This is because when comparing document similarity it doesn't matter about the actual numbers in the document only the fact that there is a number in the document.\n",
    "    * The third action is that for each token in the list it checks if the token represents \"6th\" or \"1st\" or \"2nd\" etc. If it does, then we replace the token with \"Nth\". This is because again we don't need to know the number only the fact that the document talks about the 'nth' of something.\n",
    "    * The fouth action is a check for some type of decimal or data value, this is what the regular expression represents. If it is a value of some kind it changes the token to \"NUM\". This is because again we don't need to know the data value just the fact there is a number in the document.\n",
    "* Step 3 - The variable \"rep\" is assigned to the output of the function \"stem\". This takes in the tokenlist, and for each token in the list it uses the \"WordNetLemmatizer\" to lemmatize the token. The lemmatize function replaces the token with the base case of the current token. The tokenlist is then returned. For example 'taking' would be just 'take'. This is done to increase the probability of matching words together that are the same and not just a lot of variations of the same word.\n",
    "* Step 4 - The variable \"rep\" is assigned to the output of the function \"filter_stopwords\". This takes in the tokenlist and for each token punctuation is removed. If the token is a stop word such as \"the\" \"an\" \"a\" etc. these are also removed otherwise the token is left unchanged. The tokenlist is then returned. This is done as punctuation and stopwords appear in all documents and therefore don't provide anything to help with document similarity. \n",
    "* Step 5 - A dictionary is created and for each token in the list \"rep\". The \"get\" statement looks for the value of the key for that token, if found, assigns the current value of that token's key plus 1, or if the key is not found it returns 0 plus 1. This therefore creates a dictionary of the amount of times each token is seen in the document, or tokenlist. This is exceedingly helpful as it allows us to show the most common tokens in the document and compare with other documents or the average occurances across different documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b). Apply a TF-IDF weighting to the representations and then compute: \n",
    "* the average cosine similarity of medline documents to each other, \n",
    "* the average cosine similarity of WSJ documents to each other,\n",
    "* the average cosine similarity of medline documents to WSJ documents\n",
    "\\[8 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute data #\n",
    "# word frequency in a document #\n",
    "def doc_freq(doclist):\n",
    "    df={}\n",
    "    for doc in doclist:\n",
    "        for feat in doc.keys():\n",
    "            df[feat]=df.get(feat,0)+1\n",
    "            \n",
    "    return df\n",
    "import math\n",
    "# gets the idf values for each feature in the documents # \n",
    "def idf_values(doclist):\n",
    "    idf = {}\n",
    "    n = len(doclist)\n",
    "    df = doc_freq(doclist)\n",
    "    for feat in df.keys():\n",
    "        idf[feat] = math.log10(n/(df.get(feat,0)))\n",
    "    return idf\n",
    "# converts the idf values to tfidf using the doclist as reference # \n",
    "def convert_to_tfidf(doclist,idf):\n",
    "    tfidf = []\n",
    "    for doc in doclist:\n",
    "        cur_tfidf = {}\n",
    "        for feat in idf.keys():\n",
    "            tfidf_val = doc.get(feat,0) * idf.get(feat,0)\n",
    "            if tfidf_val != 0:\n",
    "                cur_tfidf[feat] = tfidf_val\n",
    "        tfidf.append(cur_tfidf)\n",
    "    return tfidf\n",
    "# dot product of two documents #\n",
    "def dot(docA,docB):\n",
    "    the_sum=0\n",
    "    for (key,value) in docA.items():\n",
    "        the_sum+=value*docB.get(key,0)\n",
    "    return the_sum\n",
    "# cosine simularity of two documents # \n",
    "def cos_sim(docA,docB):\n",
    "    return dot(docA,docB)/(math.sqrt(dot(docA,docA) * dot(docB,docB)))\n",
    "# average given values and a length\n",
    "def average(length,values): \n",
    "    return sum(values)/length\n",
    "# finds the cosine similarity between two collections #\n",
    "def sim_collection(collectionA,collectionB):\n",
    "    values = []\n",
    "    total = 0\n",
    "    for docA in collectionA:\n",
    "        for docB in collectionB:\n",
    "            current = cos_sim(docA,docB)\n",
    "            values.append(current)\n",
    "            total += 1\n",
    "    return average(len(values), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to tfidf #\n",
    "wsj_tfidf = convert_to_tfidf(bow_collections[\"wsj\"], idf_values(bow_collections[\"wsj\"]))\n",
    "medline_tfidf = convert_to_tfidf(bow_collections[\"medline\"], idf_values(bow_collections[\"medline\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medline to Medline Cosine Similarity: 0.042554082780741256\n",
      "WSJ to WSJ Cosine Similarity: 0.042765676237362854\n",
      "Medline to WSJ Cosine Similarity: 0.007138281221940395\n"
     ]
    }
   ],
   "source": [
    "# Calculating cosine similarity #\n",
    "print(\"Medline to Medline Cosine Similarity: {}\".format(sim_collection(medline_tfidf,medline_tfidf)))\n",
    "print(\"WSJ to WSJ Cosine Similarity: {}\".format(sim_collection(wsj_tfidf,wsj_tfidf)))\n",
    "print(\"Medline to WSJ Cosine Similarity: {}\".format(sim_collection(medline_tfidf,wsj_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c). Expand the document representations by adding **synonyms** and **hypernyms** for each **noun** in the document.  For example, 2 occurrences of the word *tiger* should add 2 occurrences of each of the following **lemma_names** found in the WordNet hypernym hierarchy above *tiger*:\n",
    "* \\['tiger', 'Panthera_tigris'\\]\n",
    "* \\['big_cat', 'cat'\\]\n",
    "* \\['feline', 'felid'\\]\n",
    "* \\['carnivore'\\]\n",
    "* \\['placental', 'placental_mammal', 'eutherian', 'eutherian_mammal'\\]\n",
    "* \\['mammal', 'mammalian'\\]\n",
    "* \\['vertebrate', 'craniate'\\]\n",
    "* \\['chordate'\\]\n",
    "* \\['animal', 'animate_being', 'beast', 'brute', 'creature', 'fauna'\\]\n",
    "* \\['organism', 'being'\\]\n",
    "* \\['living_thing', 'animate_thing'\\]\n",
    "* \\['whole', 'unit'\\]\n",
    "* \\['object', 'physical_object'\\]\n",
    "* \\['physical_entity'\\]\n",
    "* \\['entity'\\]\n",
    "\n",
    "Recompute the similarities calculated in part b).  Discuss your results. \\[9 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports wordnet and sets up noun set to work out if word is a noun #\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "nouns = {word.name().split('.', 1)[0] for word in wn.all_synsets('n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add all synonyms to the document tree #\n",
    "def get_all_synonyms(syn, list_words):\n",
    "    if syn.hypernyms() != list():\n",
    "        syn_list = syn.hypernyms()\n",
    "        list_words += syn_list[0].lemma_names()\n",
    "        get_all_synonyms(syn_list[0], list_words)\n",
    "    return list_words\n",
    "\n",
    "# Function to easily add all new words to dictionary #\n",
    "def add_words(corpus, doc_count, add_list, occurs):\n",
    "    for word in add_list:\n",
    "        new_bow_collections[corpus][doc_count][word] = new_bow_collections[corpus][doc_count].get(word, 0) + occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to bow_collections # \n",
    "new_bow_collections = {key:[make_bow(doc) for doc in collection] for key,collection in collections.items()}\n",
    "for corpus in bow_collections.keys():\n",
    "    doc_count = 0\n",
    "    for doc in bow_collections[corpus]:\n",
    "        for word in doc.keys():\n",
    "            if word in nouns:\n",
    "                synsets = wn.synsets(word,wn.NOUN)\n",
    "                occurs = doc[word]\n",
    "                for syn in synsets:\n",
    "                    add_words(corpus,doc_count,syn.lemma_names(), occurs)\n",
    "                    add_words(corpus,doc_count,get_all_synonyms(syn, list()),occurs)\n",
    "        doc_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to tfidf #\n",
    "wsj_tfidf = convert_to_tfidf(new_bow_collections[\"wsj\"], idf_values(new_bow_collections[\"wsj\"]))\n",
    "medline_tfidf = convert_to_tfidf(new_bow_collections[\"medline\"], idf_values(new_bow_collections[\"medline\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medline to Medline Cosine Similarity: 0.07991586613911628\n",
      "WSJ to WSJ Cosine Similarity: 0.06693598218944792\n",
      "Medline to WSJ Cosine Similarity: 0.027670734115524086\n"
     ]
    }
   ],
   "source": [
    "# Calculating cosine similarity #\n",
    "print(\"Medline to Medline Cosine Similarity: {}\".format(sim_collection(medline_tfidf,medline_tfidf)))\n",
    "print(\"WSJ to WSJ Cosine Similarity: {}\".format(sim_collection(wsj_tfidf,wsj_tfidf)))\n",
    "print(\"Medline to WSJ Cosine Similarity: {}\".format(sim_collection(medline_tfidf,wsj_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "The results show all similarities have increased from those in Question 1.2. This is due to the increase in similar words, the most common being 'entity'. However, synonyms, such as the word 'tiger' referring to the animal or the personality trait, are not distinguished and the code above adds all trees of senses for such a word. \n",
    "\n",
    "The accuracy of the cosine similarity would increase if we could identify the correct sense for each word in the document. The calculation would be based on the addition of trees related to that sense, not all alternative senses for a word and their associated trees. However, this does not necessarily mean that the cosine similarity will increase or decrease if we use the direct senses. It would just mean that the cosine similarity would be more accurate than the current similarity calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Supervised Methods for WSD (25 marks)\n",
    "The objective of this question is to build and evaluate a word sense disambiguation (WSD) system for words with multiple senses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a).  For each word occurring in the medline corpus (defined above), **write code** to find how many senses it has according to WordNet.  Print a list of the 10 most frequently occurring words with 2 senses (in this corpus). \\[4 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words with 2 senses:\n",
      "1. membrane\n",
      "2. molecular\n",
      "3. temperature\n",
      "4. p\n",
      "5. iii\n",
      "6. mph\n",
      "7. uptake\n",
      "8. may\n",
      "9. amino\n",
      "10. molecule\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "def senses_dictionary(collection):\n",
    "    words = {} # dictionary with key being word and value being a tuple or times seen and a list of senses\n",
    "    for doc in collection:\n",
    "        for word in doc.keys():\n",
    "                if words.get(word,0) == 0:\n",
    "                    words[word] = ( wn.synsets(word), doc[word])\n",
    "                else:\n",
    "                    words[word] = (words.get(word,0)[0],doc[word] + words.get(word,0)[1])\n",
    "    return words\n",
    "# each word with each of its senses as the values and the amount it occurs in the corpus\n",
    "sense_dict = senses_dictionary(bow_collections['medline'])\n",
    "# Find two sense words and remove \n",
    "words_2_senses = {}\n",
    "for word in sense_dict.keys():\n",
    "    if len(sense_dict[word][0]) == 2:\n",
    "        words_2_senses[word] = sense_dict[word]\n",
    "# Sort words to display top 10\n",
    "words_2_senses = sorted(words_2_senses.items(),key=lambda word:word[1][1] ,reverse=True)\n",
    "print(\"Most frequent words with 2 senses:\")\n",
    "for i in range(10):\n",
    "    print(\"{}. \".format(i+1) + words_2_senses[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b). A *supervised* WSD algorithm derives model(s) from *sense-annotated corpus data* in order to predict senses of ambiguous words in un-annotated data.  Using the entire document as context, **implement** a supervised word sense disambiguation algorithm to determine the most likely sense of each occurrence of the 3 most frequently occuring words identified in part a). \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WSD Algorithm\n",
    "\n",
    "The algorithm below uses Semcor, the sense-tagged subsection of the Brown Corpus. Semcor is sense-tagged by humans, thus the sense-tagging is believed to be correct. The use of Semcor in this case as the Medline corpus does not have its own sense-tagged sub-section. If it had then the algorithm would have used this data instead.\n",
    "Semcor is setup in the proper way by the use of the functions below derived from Lab 6.1. However, there is some added function in the code block below that allows the creation of a classifier and the selection of only needed documents that have the given word.\n",
    "\n",
    "The Semcor sentences are selected at random and tagged appropriately. The use of all the documents in Semcor makes sure that even though they aren't from the same corpus the calssifier knows what the different senses of the selected word relate to. These are now tagged and fed into the classifier for that word. The classifier establishes what sentences are related to the given word sense and what sentences aren't. The algorithm then gathers a list of the documents that contain the current word and collates them into a list for more efficient classification. Each document in the list is then classified and the result, which is the most likely sense for that document, is printed along with the sentences related to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used to implement code below and imports # \n",
    "from nltk.corpus import semcor\n",
    "import random\n",
    "import nltk\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "def extract_tags(taggedsentence):\n",
    "    '''\n",
    "    For a tagged sentence in SemCor, identify single words which have been tagged with a WN synset\n",
    "    taggedsentence: a list of items, some of which are of type wordnet.tree.Tree\n",
    "    :return: a list of pairs, (word,synset)\n",
    "    \n",
    "    '''\n",
    "    alist=[]\n",
    "    for item in taggedsentence:\n",
    "        if isinstance(item,nltk.tree.Tree):   #check with this is a Tree\n",
    "            if isinstance(item.label(),nltk.corpus.reader.wordnet.Lemma) and len(item.leaves())==1:\n",
    "                #check whether the tree's label is Lemma and whether the tree has a single leaf\n",
    "                #if so add the pair (lowercased leaf,synsetlabel) to output list\n",
    "                alist.append((item.leaves()[0].lower(),item.label().synset()))\n",
    "    return alist\n",
    "            \n",
    "\n",
    "def extract_sentences(fileid_list):\n",
    "    '''\n",
    "    apply extract_tags to all sentences in all documents in a list of file ids\n",
    "    fileid_list: list of ids\n",
    "    :return: list of list of (token,tag) pairs, one for each sentence in corpus\n",
    "    '''\n",
    "    sentences=[]\n",
    "    for fileid in fileid_list:\n",
    "        print(\"Processing {}\".format(fileid))\n",
    "        sentences+=[extract_tags(taggedsentence) for taggedsentence in semcor.tagged_sents(fileid,tag='sem')]\n",
    "    return sentences\n",
    "\n",
    "def contains(sentence,astring):\n",
    "    '''\n",
    "    check whether sentence contains astring\n",
    "    '''\n",
    "    if len(sentence)>0:\n",
    "        tokens,tags=zip(*sentence)\n",
    "        return astring in tokens\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_label(sentence,word):\n",
    "    '''\n",
    "    get the synset label for the word in this sentence\n",
    "    '''\n",
    "    count=0\n",
    "    label=\"none\"\n",
    "    for token,tag in sentence:\n",
    "        if token==word:\n",
    "            count+=1\n",
    "            label=str(tag)\n",
    "    if count !=1:\n",
    "        #print(\"Warning: {} occurs {} times in {}\".format(word,count,sentence))\n",
    "        pass\n",
    "    return label\n",
    "\n",
    "def get_word_data(sentences,word):\n",
    "    '''\n",
    "    select sentences containing words and construct labelled data set where each sentence is represented using Bernouilli event model\n",
    "    '''\n",
    "    selected_sentences=[sentence for sentence in sentences if contains(sentence,word)]\n",
    "    word_data=[({token:True for (token,tag) in sentence},get_label(sentence,word)) for sentence in selected_sentences] \n",
    "    return word_data\n",
    "\n",
    "# function to create and train classifier # \n",
    "def classifier_create(training_sent, myword):\n",
    "    training=get_word_data(training_sent,myword)\n",
    "    return NaiveBayesClassifier.train(training), len(training)\n",
    "\n",
    "# function to get the specific documents for the specified word to test the data upon #\n",
    "def find_testing_sentences(collection, myword):\n",
    "    testlist = []\n",
    "    for doc in collection:\n",
    "        if myword in doc.keys():\n",
    "            testlist.append(doc)\n",
    "    return testlist\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing brownv/tagfiles/br-b21.xml\n",
      "Processing brown1/tagfiles/br-p01.xml\n",
      "Processing brown2/tagfiles/br-j29.xml\n",
      "Processing brownv/tagfiles/br-h07.xml\n",
      "Processing brown2/tagfiles/br-p24.xml\n",
      "Processing brownv/tagfiles/br-a03.xml\n",
      "Processing brownv/tagfiles/br-j24.xml\n",
      "Processing brown1/tagfiles/br-j05.xml\n",
      "Processing brownv/tagfiles/br-g05.xml\n",
      "Processing brown1/tagfiles/br-k07.xml\n",
      "Processing brownv/tagfiles/br-j25.xml\n",
      "Processing brownv/tagfiles/br-a43.xml\n",
      "Processing brown2/tagfiles/br-f08.xml\n",
      "Processing brown2/tagfiles/br-l13.xml\n",
      "Processing brownv/tagfiles/br-a09.xml\n",
      "Processing brown1/tagfiles/br-j10.xml\n",
      "Processing brown1/tagfiles/br-e24.xml\n",
      "Processing brown2/tagfiles/br-p12.xml\n",
      "Processing brownv/tagfiles/br-j27.xml\n",
      "Processing brown2/tagfiles/br-h18.xml\n",
      "Processing brownv/tagfiles/br-g09.xml\n",
      "Processing brownv/tagfiles/br-m03.xml\n",
      "Processing brown2/tagfiles/br-j33.xml\n",
      "Processing brownv/tagfiles/br-a33.xml\n",
      "Processing brownv/tagfiles/br-a37.xml\n",
      "Processing brown1/tagfiles/br-a01.xml\n",
      "Processing brown1/tagfiles/br-j14.xml\n",
      "Processing brownv/tagfiles/br-b12.xml\n",
      "Processing brownv/tagfiles/br-a38.xml\n",
      "Processing brown1/tagfiles/br-k14.xml\n",
      "Processing brown1/tagfiles/br-k09.xml\n",
      "Processing brown2/tagfiles/br-j42.xml\n",
      "Processing brown1/tagfiles/br-e01.xml\n",
      "Processing brown1/tagfiles/br-a11.xml\n",
      "Processing brown2/tagfiles/br-f21.xml\n",
      "Processing brown1/tagfiles/br-k04.xml\n",
      "Processing brown2/tagfiles/br-j32.xml\n",
      "Processing brown1/tagfiles/br-g15.xml\n",
      "Processing brown1/tagfiles/br-j02.xml\n",
      "Processing brown1/tagfiles/br-h01.xml\n",
      "Processing brown1/tagfiles/br-a15.xml\n",
      "Processing brown1/tagfiles/br-j13.xml\n",
      "Processing brownv/tagfiles/br-a36.xml\n",
      "Processing brown2/tagfiles/br-l17.xml\n",
      "Processing brown1/tagfiles/br-j57.xml\n",
      "Processing brown1/tagfiles/br-j70.xml\n",
      "Processing brown2/tagfiles/br-h13.xml\n",
      "Processing brownv/tagfiles/br-e20.xml\n",
      "Processing brown2/tagfiles/br-l10.xml\n",
      "Processing brownv/tagfiles/br-d08.xml\n",
      "Processing brown1/tagfiles/br-f03.xml\n",
      "Processing brown2/tagfiles/br-h17.xml\n",
      "Processing brown1/tagfiles/br-j15.xml\n",
      "Processing brown2/tagfiles/br-p10.xml\n",
      "Processing brownv/tagfiles/br-h04.xml\n",
      "Processing brown2/tagfiles/br-n09.xml\n",
      "Processing brownv/tagfiles/br-n08.xml\n",
      "Processing brown1/tagfiles/br-r08.xml\n",
      "Processing brownv/tagfiles/br-b23.xml\n",
      "Processing brown2/tagfiles/br-f18.xml\n",
      "Processing brownv/tagfiles/br-b05.xml\n",
      "Processing brown1/tagfiles/br-e21.xml\n",
      "Processing brown2/tagfiles/br-f20.xml\n",
      "Processing brownv/tagfiles/br-g06.xml\n",
      "Processing brownv/tagfiles/br-e08.xml\n",
      "Processing brownv/tagfiles/br-a40.xml\n",
      "Processing brown1/tagfiles/br-d01.xml\n",
      "Processing brownv/tagfiles/br-n01.xml\n",
      "Processing brownv/tagfiles/br-b16.xml\n",
      "Processing brown2/tagfiles/br-g23.xml\n",
      "Processing brownv/tagfiles/br-a07.xml\n",
      "Processing brownv/tagfiles/br-c07.xml\n",
      "Processing brownv/tagfiles/br-l04.xml\n",
      "Processing brown1/tagfiles/br-r07.xml\n",
      "Processing brown1/tagfiles/br-k17.xml\n",
      "Processing brown2/tagfiles/br-g16.xml\n",
      "Processing brownv/tagfiles/br-a21.xml\n",
      "Processing brownv/tagfiles/br-a08.xml\n",
      "Processing brownv/tagfiles/br-e17.xml\n",
      "Processing brown1/tagfiles/br-j55.xml\n",
      "Processing brownv/tagfiles/br-e15.xml\n",
      "Processing brownv/tagfiles/br-m04.xml\n",
      "Processing brownv/tagfiles/br-b02.xml\n",
      "Processing brownv/tagfiles/br-c15.xml\n",
      "Processing brown1/tagfiles/br-k15.xml\n",
      "Processing brown2/tagfiles/br-j30.xml\n",
      "Processing brownv/tagfiles/br-c16.xml\n",
      "Processing brownv/tagfiles/br-e18.xml\n",
      "Processing brownv/tagfiles/br-a17.xml\n",
      "Processing brown1/tagfiles/br-j17.xml\n",
      "Processing brown1/tagfiles/br-k08.xml\n",
      "Processing brownv/tagfiles/br-d10.xml\n",
      "Processing brownv/tagfiles/br-g08.xml\n",
      "Processing brown1/tagfiles/br-k20.xml\n",
      "Processing brownv/tagfiles/br-f04.xml\n",
      "Processing brownv/tagfiles/br-a31.xml\n",
      "Processing brown1/tagfiles/br-f10.xml\n",
      "Processing brownv/tagfiles/br-d17.xml\n",
      "Processing brownv/tagfiles/br-g13.xml\n",
      "Processing brown2/tagfiles/br-l09.xml\n",
      "Processing brown2/tagfiles/br-f17.xml\n",
      "Processing brown1/tagfiles/br-j23.xml\n",
      "Processing brown2/tagfiles/br-e28.xml\n",
      "Processing brownv/tagfiles/br-c10.xml\n",
      "Processing brown1/tagfiles/br-j22.xml\n",
      "Processing brownv/tagfiles/br-e06.xml\n",
      "Processing brown2/tagfiles/br-h12.xml\n",
      "Processing brownv/tagfiles/br-a34.xml\n",
      "Processing brownv/tagfiles/br-r02.xml\n",
      "Processing brownv/tagfiles/br-l05.xml\n",
      "Processing brownv/tagfiles/br-n07.xml\n",
      "Processing brownv/tagfiles/br-a10.xml\n",
      "Processing brown2/tagfiles/br-f14.xml\n",
      "Processing brown2/tagfiles/br-e31.xml\n",
      "Processing brownv/tagfiles/br-d12.xml\n",
      "Processing brownv/tagfiles/br-b26.xml\n",
      "Processing brownv/tagfiles/br-a22.xml\n",
      "Processing brown2/tagfiles/br-j31.xml\n",
      "Processing brown1/tagfiles/br-r06.xml\n",
      "Processing brownv/tagfiles/br-n03.xml\n",
      "Processing brownv/tagfiles/br-m06.xml\n",
      "Processing brownv/tagfiles/br-a29.xml\n",
      "Processing brown2/tagfiles/br-n20.xml\n",
      "Processing brown1/tagfiles/br-k22.xml\n",
      "Processing brown1/tagfiles/br-j59.xml\n",
      "Processing brownv/tagfiles/br-a25.xml\n",
      "Processing brownv/tagfiles/br-f12.xml\n",
      "Processing brown1/tagfiles/br-f19.xml\n",
      "Processing brownv/tagfiles/br-p08.xml\n",
      "Processing brown1/tagfiles/br-e02.xml\n",
      "Processing brownv/tagfiles/br-b03.xml\n",
      "Processing brown1/tagfiles/br-j12.xml\n",
      "Processing brown2/tagfiles/br-g17.xml\n",
      "Processing brownv/tagfiles/br-f07.xml\n",
      "Processing brownv/tagfiles/br-c09.xml\n",
      "Processing brown1/tagfiles/br-a13.xml\n",
      "Processing brown1/tagfiles/br-c01.xml\n",
      "Processing brownv/tagfiles/br-n02.xml\n",
      "Processing brown2/tagfiles/br-e30.xml\n",
      "Processing brownv/tagfiles/br-c11.xml\n",
      "Processing brownv/tagfiles/br-g03.xml\n",
      "Processing brownv/tagfiles/br-b14.xml\n",
      "Processing brown2/tagfiles/br-g20.xml\n",
      "Processing brownv/tagfiles/br-e03.xml\n",
      "Processing brown1/tagfiles/br-l12.xml\n",
      "Processing brownv/tagfiles/br-a26.xml\n",
      "Processing brown2/tagfiles/br-g28.xml\n",
      "Processing brown2/tagfiles/br-g19.xml\n",
      "Processing brown2/tagfiles/br-f22.xml\n",
      "Processing brown1/tagfiles/br-k13.xml\n",
      "Processing brown2/tagfiles/br-n15.xml\n",
      "Processing brownv/tagfiles/br-d15.xml\n",
      "Processing brown1/tagfiles/br-k03.xml\n",
      "Processing brown2/tagfiles/br-r04.xml\n",
      "Processing brownv/tagfiles/br-b15.xml\n",
      "Processing brown2/tagfiles/br-g14.xml\n",
      "Processing brownv/tagfiles/br-e16.xml\n",
      "Processing brownv/tagfiles/br-p04.xml\n",
      "Processing brown1/tagfiles/br-j04.xml\n",
      "Processing brown1/tagfiles/br-l11.xml\n",
      "Processing brownv/tagfiles/br-a27.xml\n",
      "Processing brownv/tagfiles/br-a32.xml\n",
      "Processing brown2/tagfiles/br-j41.xml\n",
      "Processing brown1/tagfiles/br-g11.xml\n",
      "Processing brown2/tagfiles/br-h15.xml\n",
      "Processing brownv/tagfiles/br-f11.xml\n",
      "Processing brown1/tagfiles/br-k11.xml\n",
      "Processing brownv/tagfiles/br-f06.xml\n",
      "Processing brownv/tagfiles/br-r01.xml\n",
      "Processing brown2/tagfiles/br-g12.xml\n",
      "Processing brownv/tagfiles/br-n06.xml\n",
      "Processing brownv/tagfiles/br-b01.xml\n",
      "Processing brown2/tagfiles/br-h16.xml\n",
      "Processing brownv/tagfiles/br-p06.xml\n",
      "Processing brown1/tagfiles/br-c04.xml\n",
      "Processing brown1/tagfiles/br-a14.xml\n",
      "Processing brown1/tagfiles/br-j06.xml\n",
      "Processing brownv/tagfiles/br-j21.xml\n",
      "Processing brownv/tagfiles/br-e13.xml\n",
      "Processing brown1/tagfiles/br-k01.xml\n",
      "Processing brownv/tagfiles/br-p05.xml\n",
      "Processing brown1/tagfiles/br-j03.xml\n",
      "Processing brown2/tagfiles/br-g39.xml\n",
      "Processing brownv/tagfiles/br-a19.xml\n",
      "Processing brown1/tagfiles/br-j08.xml\n",
      "Processing brownv/tagfiles/br-c06.xml\n",
      "Processing brownv/tagfiles/br-m05.xml\n",
      "Processing brownv/tagfiles/br-g10.xml\n",
      "Processing brown2/tagfiles/br-j35.xml\n",
      "Processing brown1/tagfiles/br-j56.xml\n",
      "Processing brown2/tagfiles/br-e22.xml\n",
      "Processing brownv/tagfiles/br-c17.xml\n",
      "Processing brownv/tagfiles/br-e11.xml\n",
      "Processing brownv/tagfiles/br-b11.xml\n",
      "Processing brown2/tagfiles/br-l18.xml\n",
      "Processing brownv/tagfiles/br-d05.xml\n",
      "Processing brown1/tagfiles/br-j37.xml\n",
      "Processing brown1/tagfiles/br-k10.xml\n",
      "Processing brownv/tagfiles/br-f05.xml\n",
      "Processing brownv/tagfiles/br-j28.xml\n",
      "Processing brown1/tagfiles/br-j20.xml\n",
      "Processing brownv/tagfiles/br-b09.xml\n",
      "Processing brown2/tagfiles/br-e26.xml\n",
      "Processing brown1/tagfiles/br-j07.xml\n",
      "Processing brownv/tagfiles/br-f09.xml\n",
      "Processing brownv/tagfiles/br-e19.xml\n",
      "Processing brownv/tagfiles/br-d11.xml\n",
      "Processing brownv/tagfiles/br-d07.xml\n",
      "Processing brownv/tagfiles/br-e12.xml\n",
      "Processing brownv/tagfiles/br-b10.xml\n",
      "Processing brown1/tagfiles/br-j60.xml\n",
      "Processing brown2/tagfiles/br-g43.xml\n",
      "Processing brown1/tagfiles/br-f43.xml\n",
      "Processing brownv/tagfiles/br-h03.xml\n",
      "Processing brownv/tagfiles/br-a05.xml\n",
      "Processing brownv/tagfiles/br-b07.xml\n",
      "Processing brown1/tagfiles/br-m01.xml\n",
      "Processing brown1/tagfiles/br-k23.xml\n",
      "Processing brown2/tagfiles/br-g22.xml\n",
      "Processing brownv/tagfiles/br-a23.xml\n",
      "Processing brown1/tagfiles/br-k25.xml\n",
      "Processing brown2/tagfiles/br-n11.xml\n",
      "Processing brown2/tagfiles/br-j34.xml\n",
      "Processing brown1/tagfiles/br-j52.xml\n",
      "Processing brown2/tagfiles/br-f24.xml\n",
      "Processing brownv/tagfiles/br-b25.xml\n",
      "Processing brownv/tagfiles/br-d16.xml\n",
      "Processing brown2/tagfiles/br-f23.xml\n",
      "Processing brownv/tagfiles/br-f01.xml\n",
      "Processing brownv/tagfiles/br-h06.xml\n",
      "Processing brown2/tagfiles/br-n14.xml\n",
      "Processing brown1/tagfiles/br-k19.xml\n",
      "Processing brown2/tagfiles/br-g18.xml\n",
      "Processing brownv/tagfiles/br-b17.xml\n",
      "Processing brown1/tagfiles/br-k27.xml\n",
      "Processing brownv/tagfiles/br-a44.xml\n",
      "Processing brownv/tagfiles/br-a04.xml\n",
      "Processing brown2/tagfiles/br-n17.xml\n",
      "Processing brown1/tagfiles/br-j58.xml\n",
      "Processing brownv/tagfiles/br-h02.xml\n",
      "Processing brownv/tagfiles/br-h10.xml\n",
      "Processing brown2/tagfiles/br-f33.xml\n",
      "Processing brownv/tagfiles/br-l07.xml\n",
      "Processing brown1/tagfiles/br-j16.xml\n",
      "Processing brownv/tagfiles/br-h08.xml\n",
      "Processing brownv/tagfiles/br-a35.xml\n",
      "Processing brown2/tagfiles/br-n12.xml\n",
      "Processing brown1/tagfiles/br-d03.xml\n",
      "Processing brown2/tagfiles/br-f16.xml\n",
      "Processing brownv/tagfiles/br-a39.xml\n",
      "Processing brown2/tagfiles/br-l15.xml\n",
      "Processing brownv/tagfiles/br-r03.xml\n",
      "Processing brownv/tagfiles/br-b08.xml\n",
      "Processing brown1/tagfiles/br-k06.xml\n",
      "Processing brownv/tagfiles/br-a18.xml\n",
      "Processing brownv/tagfiles/br-a16.xml\n",
      "Processing brown1/tagfiles/br-j54.xml\n",
      "Processing brown1/tagfiles/br-b20.xml\n",
      "Processing brown2/tagfiles/br-p09.xml\n",
      "Processing brownv/tagfiles/br-b24.xml\n",
      "Processing brown2/tagfiles/br-l14.xml\n",
      "Processing brownv/tagfiles/br-l02.xml\n",
      "Processing brown1/tagfiles/br-j19.xml\n",
      "Processing brown1/tagfiles/br-k05.xml\n",
      "Processing brownv/tagfiles/br-e09.xml\n",
      "Processing brown2/tagfiles/br-h09.xml\n",
      "Processing brown1/tagfiles/br-k29.xml\n",
      "Processing brownv/tagfiles/br-b22.xml\n",
      "Processing brownv/tagfiles/br-a28.xml\n",
      "Processing brownv/tagfiles/br-b27.xml\n",
      "Processing brown1/tagfiles/br-k21.xml\n",
      "Processing brown1/tagfiles/br-j18.xml\n",
      "Processing brownv/tagfiles/br-l06.xml\n",
      "Processing brownv/tagfiles/br-e07.xml\n",
      "Processing brown2/tagfiles/br-n16.xml\n",
      "Processing brownv/tagfiles/br-c12.xml\n",
      "Processing brown2/tagfiles/br-h24.xml\n",
      "Processing brown2/tagfiles/br-e23.xml\n",
      "Processing brown2/tagfiles/br-j38.xml\n",
      "Processing brownv/tagfiles/br-j26.xml\n",
      "Processing brown1/tagfiles/br-m02.xml\n",
      "Processing brownv/tagfiles/br-c08.xml\n",
      "Processing brown1/tagfiles/br-r09.xml\n",
      "Processing brown1/tagfiles/br-a12.xml\n",
      "Processing brown2/tagfiles/br-h11.xml\n",
      "Processing brown2/tagfiles/br-f13.xml\n",
      "Processing brownv/tagfiles/br-a06.xml\n",
      "Processing brownv/tagfiles/br-f02.xml\n",
      "Processing brownv/tagfiles/br-a42.xml\n",
      "Processing brownv/tagfiles/br-n04.xml\n",
      "Processing brown2/tagfiles/br-h21.xml\n",
      "Processing brownv/tagfiles/br-d14.xml\n",
      "Processing brownv/tagfiles/br-a30.xml\n",
      "Processing brownv/tagfiles/br-a24.xml\n",
      "Processing brownv/tagfiles/br-b19.xml\n",
      "Processing brownv/tagfiles/br-d09.xml\n",
      "Processing brownv/tagfiles/br-d06.xml\n",
      "Processing brown1/tagfiles/br-k26.xml\n",
      "Processing brown1/tagfiles/br-a02.xml\n",
      "Processing brownv/tagfiles/br-e05.xml\n",
      "Processing brownv/tagfiles/br-b06.xml\n",
      "Processing brown1/tagfiles/br-k02.xml\n",
      "Processing brown1/tagfiles/br-b13.xml\n",
      "Processing brown1/tagfiles/br-c02.xml\n",
      "Processing brownv/tagfiles/br-c03.xml\n",
      "Processing brownv/tagfiles/br-b04.xml\n",
      "Processing brownv/tagfiles/br-d13.xml\n",
      "Processing brown1/tagfiles/br-g01.xml\n",
      "Processing brownv/tagfiles/br-l01.xml\n",
      "Processing brown2/tagfiles/br-l08.xml\n",
      "Processing brownv/tagfiles/br-c14.xml\n",
      "Processing brown2/tagfiles/br-l16.xml\n",
      "Processing brownv/tagfiles/br-a20.xml\n",
      "Processing brown1/tagfiles/br-r05.xml\n",
      "Processing brownv/tagfiles/br-b18.xml\n",
      "Processing brown1/tagfiles/br-k18.xml\n",
      "Processing brown2/tagfiles/br-f44.xml\n",
      "Processing brown2/tagfiles/br-e25.xml\n",
      "Processing brown1/tagfiles/br-j01.xml\n",
      "Processing brown1/tagfiles/br-n05.xml\n",
      "Processing brown1/tagfiles/br-k12.xml\n",
      "Processing brown2/tagfiles/br-n10.xml\n",
      "Processing brown1/tagfiles/br-k28.xml\n",
      "Processing brown1/tagfiles/br-e04.xml\n",
      "Processing brown2/tagfiles/br-f25.xml\n",
      "Processing brown1/tagfiles/br-e29.xml\n",
      "Processing brown2/tagfiles/br-e27.xml\n",
      "Processing brown1/tagfiles/br-j11.xml\n",
      "Processing brown1/tagfiles/br-k24.xml\n",
      "Processing brownv/tagfiles/br-a41.xml\n",
      "Processing brown2/tagfiles/br-p07.xml\n",
      "Processing brown2/tagfiles/br-g44.xml\n",
      "Processing brown1/tagfiles/br-k16.xml\n",
      "Processing brownv/tagfiles/br-p03.xml\n",
      "Processing brown1/tagfiles/br-d02.xml\n",
      "Processing brown1/tagfiles/br-j53.xml\n",
      "Processing brownv/tagfiles/br-c13.xml\n",
      "Processing brown2/tagfiles/br-g31.xml\n",
      "Processing brown1/tagfiles/br-d04.xml\n",
      "Processing brown2/tagfiles/br-h14.xml\n",
      "Processing brownv/tagfiles/br-l03.xml\n",
      "Processing brown2/tagfiles/br-g21.xml\n",
      "Processing brownv/tagfiles/br-e10.xml\n",
      "Processing brownv/tagfiles/br-g04.xml\n",
      "Processing brownv/tagfiles/br-p02.xml\n",
      "Processing brownv/tagfiles/br-e14.xml\n",
      "Processing brownv/tagfiles/br-h05.xml\n",
      "Processing brownv/tagfiles/br-g02.xml\n",
      "Processing brownv/tagfiles/br-g07.xml\n",
      "Processing brown1/tagfiles/br-j09.xml\n",
      "Processing brown2/tagfiles/br-f15.xml\n",
      "Processing brownv/tagfiles/br-c05.xml\n"
     ]
    }
   ],
   "source": [
    "# setting up the training sentences #\n",
    "allfiles = semcor.fileids()\n",
    "shuffled=list(allfiles)\n",
    "random.shuffle(shuffled)\n",
    "training_sentences=extract_sentences(shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membrane Document 1: ['polysaccharide', 'cause', 'inhibition', 'multiplication', 'mumps', 'virus', 'allantoic', 'sac', 'may', 'hemagglutination', 'moreover', 'substance', 'prevent', 'adsorption', 'erythrocyte', 'available', 'evidence', 'indicates', 'active', 'inhibitor', 'block', 'cell', 'living', 'membrane', 'influenza', 'b', 'newcastle', 'disease', 'well', 'pvm', 'also', 'appears', 'lack', 'correlation', 'vitro', 'vivo', 'inhibiting', 'activity']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 2: ['interaction', 'NUM', 'phage', 'strain', 'bacillus', 'protoplast', 'l', 'form', 'subtilis', 'eight', 'mutant', 'two', 'lysogens', 'described', 'qualitatively', 'quantitatively', 'removal', 'cell', 'wall', 'still', 'adsorb', 'nine', 'kill', 'host', 'five', 'multiply', 'naked', 'bacteria', 'forming', 'plaque', 'lawn', 'individual', 'gene', 'mutation', 'similarly', 'pleiotropic', 'effect', 'strongly', 'dependent', 'upon', 'plating', 'medium', 'thus', 'gta', 'cause', 'loss', 'glucosylation', 'teichoic', 'acid', 'result', 'adsorption', 'site', 'phi', 'membrane', 'phie', 'unstabilized', 'lysogenized', 'wild', 'type', 'also', 'exhibit', 'pattern', 'showing', 'different', 'level', 'resistance', 'similar', 'basis', 'bacterial', 'could', 'ordered', 'four', 'class', 'classified', 'six', 'group', 'together', 'complex', 'based', 'partly', 'perhaps', 'metabolic', 'block', 'development']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 3: ['localization', 'dehydrogenase', 'membrane', 'vesicle', 'prepared', 'escherichia', 'coli', 'wa', 'studied', 'using', 'antibody', 'purified', 'enzyme', 'activity', 'oxygen', 'uptake', 'french', 'press', 'completely', 'inhibited', 'suggesting', 'localized', 'outside', 'previous', 'result', 'futai', 'NUM', 'strongly', 'indicate', 'inversion', 'treatment', 'whereas', 'proline', 'transport', 'insensitive', 'suggest', 'inside', 'amino', 'acid', 'essentially', 'confirms', 'short', 'kaback', 'kohn', 'however', 'unlike', 'observed', 'small', 'significant', 'portion', 'sensitive', 'shown', 'may', 'represent', 'inverted', 'preparation', 'ferricyanide', 'reductase', 'detected', 'spheroplasts', 'total', 'ethylenediaminetetraacetate', 'finding', 'slightly', 'different', 'procedure', 'concluded', 'half', 'reactive', 'site', 'moved', 'remains']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 4: ['apparent', 'permeability', 'mutation', 'produced', 'escherichia', 'coli', 'bacteriophage', 'mutagenesis', 'pleiotropic', 'showing', 'sensitivity', 'number', 'detergent', 'unrelated', 'antibiotic', 'presumably', 'affect', 'cell', 'wall', 'membrane', 'biosynthesis', 'one', 'wa', 'genetically', 'mapped', 'site', 'near', 'acra', 'mtc', 'locus', 'approximately', 'NUM', 'min', 'taylor', 'trotter', 'map']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 5: ['formation', 'membrane', 'potential', 'energized', 'coli', 'cell', 'ha', 'investigated', 'mean', 'ionic', 'penetrants', 'flux', 'anion', 'cation', 'opposite', 'direction', 'observed', 'moved', 'uptake', 'wa', 'stoichiometrically', 'coupled', 'outflow', 'ion', 'value', 'calculated', 'distribution', 'permanent', 'range', 'mv', 'inside', 'minus', 'penetrating', 'deenergized', 'following', 'generation', 'influx', 'synthetic', 'natural', 'lactose', 'collapsed', 'effect', 'sensitive', 'maleimide', 'result', 'favour', 'conception', 'generates', 'driving', 'force', 'transport']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 6: ['propranolol', 'able', 'increase', 'amount', 'titratable', 'group', 'mitochondrial', 'membrane', 'effect', 'occurs', 'sonicated', 'particle', 'liposome', 'phenomenon', 'seen', 'presence', 'salt', 'solution', 'sucrose', 'fluorescence', 'sulphonate', 'suspension', 'counteracted', 'increasing', 'concentration', 'potassium', 'chloride', 'suggested', 'result', 'decrease', 'aggregation', 'phospholipid', 'time', 'environment', 'bound', 'molecule', 'hydrophobic', 'buffering', 'hydrophilicity', 'direct', 'inverse', 'correlation']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 7: ['pyruvate', 'oxidase', 'system', 'escherichia', 'coli', 'composed', 'soluble', 'flavoprotein', 'ec', 'NUM', 'ferricytochrome', 'oxidoreductase', 'electron', 'transport', 'associated', 'cell', 'fraction', 'membrane', 'particle', 'contain', 'lipid', 'weight', 'fractionation', 'revealed', 'abut', 'neutral', 'phospholipid', 'relative', 'ratio', 'ubiquinone', 'menaquinone', 'within', 'molar', 'basis', 'removal', 'extraction', 'aqueous', 'acetone', 'hydrolysis', 'treatment', 'bacillus', 'cereus', 'phospholipase', 'c', 'result', 'complete', 'loss', 'activity', 'analysis', 'extracted', 'practically', 'removed', 'phosphorus', 'however', 'diglycerides', 'produced', 'remain', 'addition', 'detergent', 'dodecylamide', 'material', 'restoration', 'original', 'completely', 'restores', 'furthermore', 'either', 'yeast', 'restore', 'quinone', 'supplemented', 'photoinactivated', 'noted', 'upon', 'reconstitution', 'fact', 'compound', 'appear', 'suppress', 'added', 'reaction', 'mixture', 'containing']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 8: ['glycerophospholipids', 'culture', 'neurospora', 'crassa', 'extracted', 'deacylated', 'analyzed', 'addition', 'strain', 'several', 'auxotrophic', 'mutant', 'examined', 'defective', 'phosphatidylethanolamine', 'methyltransferase', 'methionine', 'phosphatidylmonomethylethanolamine', 'dimethylethanolamine', 'inos', 'phosphatase', 'double', 'wa', 'constructed', 'grown', 'concentration', 'supplement', 'adequate', 'support', 'growth', 'bizarre', 'phospholipid', 'composition', 'appropriate', 'choice', 'possible', 'vary', 'relative', 'level', 'every', 'organism', 'exception', 'cardiolipin', 'maximum', 'range', 'encountered', 'zwitterionic', 'specie', 'expressed', 'per', 'cent', 'total', 'phosphorus', 'lecithin', 'NUM', 'phosphatidyldimethylethanolamine', 'anionic', 'phosphatidylserine', 'phosphatidylinositol', 'despite', 'wide', 'variation', 'proportion', 'individual', 'five', 'quantity', 'remained', 'constant', 'content', 'ratio', 'data', 'suggest', 'existence', 'internal', 'compensation', 'mechanism', 'net', 'effect', 'maintenance', 'fairly', 'contribution', 'component', 'membrane', 'charge']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 9: ['binding', 'degradation', 'gtp', 'guanosine', 'beta', 'triphosphate', 'gpp', 'nh', 'p', 'plasma', 'membrane', 'rat', 'liver', 'fat', 'cell', 'investigated', 'hydrolyzed', 'predominantly', 'nucleotide', 'pyrophosphohydrolases', 'whereas', 'primarily', 'phosphohydrolases', 'enzyme', 'specific', 'guanine', 'since', 'analogous', 'adenine', 'spare', 'hydrolysis', 'taken', 'site', 'extent', 'high', 'concentration', 'corresponding', 'fail', 'inhibit', 'uptake', 'appear', 'remains', 'essentially', 'intact', 'irrespective', 'degree', 'unbound', 'indicating', 'siteis', 'incapable', 'degrading', 'gdp', 'competitively', 'constant', 'three', 'similar', 'NUM', 'mum', 'range', 'required', 'effect', 'adenylate', 'cyclase', 'activity', 'inhibited', 'sulfhydryl', 'agent', 'suggesting', 'group', 'involved', 'process', 'contrast', 'accompanied', 'substantial', 'incubation', 'condition', 'atp', 'plus', 'regenerating', 'system', 'medium', 'bound', 'progressively', 'thus', 'although', 'differentially', 'susceptible', 'terminal', 'phosphate', 'finding', 'discussed', 'term', 'markedly', 'different', 'potency', 'activator']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "membrane Document 10: ['escherichia', 'coli', 'salmonella', 'typhimurium', 'cell', 'wall', 'contains', 'outer', 'membrane', 'layer', 'peptidoglycan', 'act', 'barrier', 'molecular', 'sieve', 'type', 'penetration', 'uncharged', 'saccharide', 'decad', 'nakae', 'nikaido', 'NUM', 'fed', 'proc', 'examined', 'limited', 'size', 'penetrating', 'molecule', 'studying', 'whose', 'destroyed', 'lysozyme', 'treatment', 'growth', 'presence', 'penicillin', 'b', 'isolated', 'vesicle', 'found', 'similar', 'intact', 'plasmolyzed', 'allowed', 'partial', 'stachyose', 'weight', 'essentially', 'excluded', 'higher', 'also', 'acted', 'observation', 'led', 'u', 'conclude', 'rather', 'set', 'limit', 'hydrophilic', 'however', 'exclusion', 'much', 'leakiness', 'could', 'decreased', 'either', 'use', 'mutant', 'producing', 'extremely', 'deficient', 'lipopolysaccharide', 'trypsin', 'followed', 'heating', 'slow', 'cooling', 'feel', 'consistent', 'hypothesis', 'resealing', 'ruptured', 'isolation', 'procedure', 'often', 'incomplete', 'crack', 'hole', 'thus', 'generated', 'responsible']\n",
      "\n",
      "Classified as: Synset('membrane.n.01')\n",
      "\n",
      "molecular Document 1: ['three', 'enzyme', 'one', 'ec', 'NUM', 'peptide', 'hydrolase', 'activity', 'separated', 'partially', 'purified', 'bacillus', 'subtilis', 'distinguished', 'respect', 'molecular', 'weight', 'catalytic', 'property', 'studied', 'relation', 'physiology', 'bacterium', 'designated', 'aminopeptidase', 'ha', 'produced', 'early', 'growth', 'hydrolyzes', 'rapidly', 'another', 'ii', 'also', 'third', 'iii', 'predominantly', 'stationary', 'phase', 'efficiently', 'utilizes', 'substrate', 'synthesis', 'suggests', 'selective', 'catabolism', 'occurs', 'time', 'perhaps', 'related', 'cessation', 'onset', 'event', 'dipeptide', 'well', 'identified', 'localized', 'cell', 'wall', 'periplasm', 'organism', 'evidence', 'variation', 'cycle', 'suggest', 'important', 'function', 'antibiotic', 'metabolism']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "molecular Document 2: ['two', 'eighteen', 'strain', 'bacillus', 'subtilis', 'examined', 'contained', 'covalently', 'closed', 'circular', 'duplex', 'deoxyribonucleic', 'acid', 'dna', 'homogeneous', 'size', 'buoyant', 'density', 'atcc', 'NUM', 'copy', 'per', 'chromosome', 'plasmid', 'element', 'molecular', 'weight', 'time', 'one', 'appeared', 'closely', 'related', 'genetic', 'physiological', 'biochemical', 'criterion', 'much', 'le', 'pumilus', 'class', 'respectively', 'present', 'several']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "molecular Document 3: ['two', 'clinical', 'isolates', 'bacteroides', 'contained', 'covalently', 'closed', 'circular', 'deoxyribonucleic', 'acid', 'dna', 'shown', 'sedimentation', 'alkaline', 'sucrose', 'gradient', 'cscl', 'ethidium', 'bromide', 'equilibrium', 'centrifugation', 'electron', 'microscopy', 'bacteriodes', 'fragilis', 'homogeneous', 'specie', 'plasmid', 'molecular', 'weight', 'NUM', 'x', 'ochraceus', 'distinct', 'element', 'larger', 'cosedimented', 'form', 'r', 'corresponding', 'smaller', 'sedimented', 'molecule', 'calculated', 'role', 'unknown', 'neither', 'strain', 'transferred', 'antibiotic', 'resistance', 'escherichia', 'coli', 'produced', 'bacteriocins', 'active', 'sensitive', 'indicator']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "molecular Document 4: ['human', 'liver', 'ha', 'purified', 'apparent', 'homogeneity', 'NUM', 'yield', 'affinity', 'chromatographic', 'procedure', 'utilizing', 'agarose', 'isoelectric', 'focusing', 'revealed', 'six', 'form', 'enzyme', 'polyacrylamide', 'gel', 'electrophoresis', 'demonstrated', 'presence', 'band', 'protein', 'contained', 'fucosidase', 'activity', 'preparation', 'wa', 'found', 'contain', 'trace', 'amount', 'seven', 'glycosidases', 'quantitative', 'amino', 'acid', 'analysis', 'performed', 'preliminary', 'carbohydrate', 'indicated', 'molecule', 'filtration', 'sepharose', 'approximate', 'molecular', 'weight', 'high', 'speed', 'sedimentation', 'equilibrium', 'yielded', 'sodium', 'dodecyl', 'sulfate', 'single', 'subunit', 'ph', 'optimum', 'suggested', 'second', 'michaelis', 'constant', 'maximal', 'velocity', 'determined', 'respect', 'substrate', 'mm', 'respectively', 'several', 'salt', 'little', 'effect', 'although', 'completely', 'inactivated', 'antibody', 'made', 'dound', 'monospecific', 'crude', 'supernatant', 'fluid', 'pure', 'antigen', 'material', 'detected', 'patient', 'died', 'fucosidosis']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "molecular Document 5: ['three', 'fragment', 'specie', 'isolated', 'time', 'plasmin', 'digest', 'fibrinogen', 'molecular', 'weight', 'summation', 'subunit', 'sodium', 'dodecyl', 'sulfate', 'polyacrylamide', 'gel', 'electrophoresis', 'sedimentation', 'equilibrium', 'ultracentrifugation', 'NUM', 'depending', 'value', 'calculated', 'partial', 'specific', 'volume', 'contained', 'derived', 'aalpha', 'bbeta', 'gamma', 'chain', 'differed', 'extent', 'degradation', 'derivative', 'cleaved', 'release', 'site', 'however', 'beta', 'alpha', 'resistant', 'digestion', 'highly', 'fibrin', 'dimeric', 'structure', 'due', 'formation', 'two', 'wa', 'well', 'incorporating', 'fluorescent', 'label', 'monodansyl', 'cadaverine', 'acceptor', 'prevented', 'therefore', 'one', 'monomer', 'generated', 'result', 'show', 'unequivocally', 'contains', 'must', 'production', 'molecule', 'recently', 'proposed', 'model', 'cleavage', 'postulate', 'generation', 'single', 'pair', 'incorrect', 'incorporation', 'alter', 'detectably', 'series', 'peptide', 'ranged', 'early', 'stage', 'degraded', 'progressively', 'brightly', 'weakly', 'thus', 'within', 'segment']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "molecular Document 6: ['thyroglobulin', 'obtained', 'guinea', 'pig', 'wa', 'examined', 'na', 'gel', 'electrophoresis', 'reduction', 'alkylation', 'contrast', 'mammalian', 'source', 'three', 'group', 'polypeptide', 'chain', 'accounted', 'NUM', 'protein', 'determination', 'molecular', 'weight', 'purified', 'equilibrium', 'centrifugation', 'guanidine', 'hcl', 'gave', 'value', 'specie', 'b', 'c', 'determined', 'filtration', 'similar', 'result', 'due', 'large', 'size', 'satisfactory', 'could', 'amino', 'acid', 'analysis', 'whole', 'slightly', 'higher', 'level', 'lysine', 'histidine', 'lower', 'glutamic', 'seen', 'iodine', 'content', 'found', 'range', 'respectively']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "molecular Document 7: ['escherichia', 'coli', 'salmonella', 'typhimurium', 'cell', 'wall', 'contains', 'outer', 'membrane', 'layer', 'peptidoglycan', 'act', 'barrier', 'molecular', 'sieve', 'type', 'penetration', 'uncharged', 'saccharide', 'decad', 'nakae', 'nikaido', 'NUM', 'fed', 'proc', 'examined', 'limited', 'size', 'penetrating', 'molecule', 'studying', 'whose', 'destroyed', 'lysozyme', 'treatment', 'growth', 'presence', 'penicillin', 'b', 'isolated', 'vesicle', 'found', 'similar', 'intact', 'plasmolyzed', 'allowed', 'partial', 'stachyose', 'weight', 'essentially', 'excluded', 'higher', 'also', 'acted', 'observation', 'led', 'u', 'conclude', 'rather', 'set', 'limit', 'hydrophilic', 'however', 'exclusion', 'much', 'leakiness', 'could', 'decreased', 'either', 'use', 'mutant', 'producing', 'extremely', 'deficient', 'lipopolysaccharide', 'trypsin', 'followed', 'heating', 'slow', 'cooling', 'feel', 'consistent', 'hypothesis', 'resealing', 'ruptured', 'isolation', 'procedure', 'often', 'incomplete', 'crack', 'hole', 'thus', 'generated', 'responsible']\n",
      "\n",
      "Classified as: Synset('molecular.a.01')\n",
      "\n",
      "temperature Document 1: ['effect', 'elevated', 'temperature', 'growth', 'rate', 'wa', 'studied', 'five', 'strain', 'enterobacteriaceae', 'tested', 'shift', 'resulted', 'immediate', 'decrease', 'due', 'limitation', 'availability', 'endogenous', 'methionine', 'first', 'biosynthetic', 'enzyme', 'extract', 'aerobacter', 'aerogenes', 'salmonella', 'typhimurium', 'escherichia', 'coli', 'shown', 'sensitive']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n",
      "temperature Document 2: ['localization', 'mutation', 'affecting', 'ribonuclease', 'iii', 'activity', 'enzyme', 'specific', 'ribonucleic', 'acid', 'escherichia', 'coli', 'wa', 'attempted', 'series', 'mating', 'transduction', 'experiment', 'mapped', 'near', 'nadb', 'gene', 'strain', 'carrying', 'another', 'also', 'found', 'based', 'available', 'data', 'order', 'chromosome', 'appears', 'tyra', 'rana', 'rnc', 'puri', 'either', 'fail', 'grow', 'NUM', 'c', 'enriched', 'medium', 'whereas', 'defective', 'slowly', 'corresponding', 'tested', 'temperature', 'reduces', 'growth', 'rate', 'bacteriophage', 'form', 'plaque', 'lower', 'efficiency', 'thus', 'suggest', 'beneficial', 'normal', 'higher', 'becomes', 'indispensable']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n",
      "temperature Document 3: ['mutant', 'escherichia', 'coli', 'conditionally', 'lethal', 'mutation', 'structural', 'gene', 'deoxyribonucleic', 'acid', 'dna', 'ligase', 'extensive', 'repair', 'synthesis', 'occurred', 'cell', 'nonpermissive', 'temperature', 'NUM', 'permissive', 'c', 'nearly', 'normal', 'semiconservative', 'limited', 'observed', 'wa', 'activated', 'addition', 'nicotinamide', 'adenine', 'dinucleotide']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n",
      "temperature Document 4: ['NUM', 'primary', 'culture', 'sarcoma', 'cell', 'exhibit', 'much', 'greater', 'activity', 'normal', 'connective', 'tissue', 'grown', 'adult', 'blood', 'vessel', 'shorter', 'latent', 'period', 'ameboid', 'phenomenon', 'marked', 'multiplication', 'proceeds', 'rapidly', 'secondary', 'le', 'active', 'hand', 'show', 'markedly', 'accelerated', 'growth', 'easily', 'propagated', 'long', 'vitro', 'multiply', 'actively', 'three', 'month', 'old', 'method', 'cultivation', 'well', 'adapted', 'study', 'pathological', 'division', 'nuclear', 'change', 'discernible', 'living', 'staining', 'may', 'applied', 'verify', 'observation', 'upon', 'unstained', 'structure', 'atypical', 'mitoses', 'several', 'kind', 'found', 'seen', 'time', 'required', 'rat', 'kept', 'body', 'temperature', 'degree', 'varies', 'within', 'relatively', 'narrow', 'limit', 'twenty', 'fifty', 'minute', 'contrary', 'variation', 'hour', 'amitotic', 'ha', 'observed', 'either', 'tumor', 'evidence', 'budding', 'however', 'formation', 'containing', 'nucleus', 'irregular', 'size', 'noted', 'development', 'two', 'mononuclear', 'mitotic', 'without', 'cytoplasm', 'also']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n",
      "temperature Document 5: ['precise', 'oxygen', 'equilibrium', 'curve', 'human', 'adult', 'hemoglobin', 'determined', 'automatic', 'recording', 'method', 'several', 'temperature', 'presence', 'absence', 'dpg', 'inositol', 'hexaphosphate', 'ihp', 'NUM', 'hydroxymethyl', 'buffer', 'ph', 'containing', 'data', 'analyzed', 'according', 'adair', 'scheme', 'heat', 'deltahi', 'entropy', 'change', 'deltasi', 'individual', 'oxygenation', 'step', 'obtained', 'shape', 'varies', 'whether', 'present', 'absent', 'consequence', 'value', 'depends', 'behavior', 'similar', 'resulted', 'compensation', 'phenomenon', 'contribution', 'cdeltasi', 'free', 'energy', 'compensated', 'first', 'three', 'fourth', 'upon', 'addition', 'accompanied', 'c', 'k', 'major', 'part', 'nonuniformity', 'appears', 'attributable', 'binding', 'result', 'necessarily', 'support', 'earlier', 'idea', 'wyman', 'cooperative', 'oxygenbinding', 'essentially', 'effect']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n",
      "temperature Document 6: ['binding', 'site', 'located', 'atpase', 'purified', 'fragmented', 'sarcoplasmic', 'reticulum', 'ikemoto', 'n', 'NUM', 'biol', 'chem', 'studied', 'degree', 'three', 'class', 'denoted', 'alpha', 'k', 'congruent', 'time', 'beta', 'gamma', 'two', 'per', 'dalton', 'one', 'change', 'reversible', 'parallelism', 'reported', 'sumida', 'tonomura', 'biochem', 'temperature', 'dependence', 'ratio', 'transport', 'atp', 'cleavage', 'suggests', 'involvement', 'study', 'low', 'enzyme', 'mol', 'g', 'unit', 'permitting', 'separate', 'investigation', 'phosphorylation', 'dephosphorylation', 'process', 'show', 'concomitantly', 'formation', 'phosphorylated', 'e', 'approximately', 'p', 'bound', 'calcium', 'released', 'rebound', 'moiety', 'inhibits', 'liberation', 'pi', 'analysis', 'use', 'hill', 'plot', 'inhibition', 'average', 'affinity', 'tentatively', 'identified', 'form']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n",
      "temperature Document 7: ['ionization', 'constant', 'NUM', 'histidine', 'residue', 'ribonuclease', 'beenobtained', 'temperature', 'nuclear', 'magnetic', 'resonance', 'titration', 'curve', 'imidazole', 'proton', 'thermodynamic', 'parameter', 'derived', 'indicate', 'fairly', 'well', 'exposed', 'solvent', 'somewhat', 'restricted', 'environment', 'measurement', 'low', 'ph', 'inflection', 'present', 'yield', 'large', 'negative', 'entropy', 'value', 'indicating', 'group', 'givine', 'rise', 'also', 'buried']\n",
      "\n",
      "Classified as: Synset('temperature.n.02')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing the classifier # \n",
    "evalulating_tests = [[] for i in range(3)]\n",
    "length_of_training_data = []\n",
    "# for each word # \n",
    "for i in range(3):\n",
    "    current_word =words_2_senses[i][0]\n",
    "    current_classifier, length = classifier_create(training_sentences,current_word)\n",
    "    length_of_training_data.append(length)\n",
    "    testing_sentences = find_testing_sentences(bow_collections['medline'],current_word)\n",
    "    doc_count = 0\n",
    "    for doc in testing_sentences:\n",
    "        doc_count += 1\n",
    "        classification = current_classifier.classify(doc)\n",
    "        evalulating_tests[i].append((list(doc.keys()),classification))\n",
    "        print(\"{} Document {}: {}\".format(current_word,doc_count,list(doc.keys())))\n",
    "        print(\"\")\n",
    "        print(\"Classified as: {}\".format(classification))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c). Evaluate the performance of your WSD system.  How accurate is it for each of the 3 words? **Comment** on the strengths and weaknesses of your WSD system.\\[8 marks\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membrane\n",
      "[Synset('membrane.n.01'), Synset('membrane.n.02')]\n",
      "1st definition: a thin pliable sheet of material\n",
      "2nd definition: a pliable sheet of tissue that covers or lines or connects the organs or cells of animals or plants\n",
      "Length of training data: 1\n",
      "\n",
      "Test 1\n",
      "['polysaccharide', 'cause', 'inhibition', 'multiplication', 'mumps', 'virus', 'allantoic', 'sac', 'may', 'hemagglutination', 'moreover', 'substance', 'prevent', 'adsorption', 'erythrocyte', 'available', 'evidence', 'indicates', 'active', 'inhibitor', 'block', 'cell', 'living', 'membrane', 'influenza', 'b', 'newcastle', 'disease', 'well', 'pvm', 'also', 'appears', 'lack', 'correlation', 'vitro', 'vivo', 'inhibiting', 'activity']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 2\n",
      "['interaction', 'NUM', 'phage', 'strain', 'bacillus', 'protoplast', 'l', 'form', 'subtilis', 'eight', 'mutant', 'two', 'lysogens', 'described', 'qualitatively', 'quantitatively', 'removal', 'cell', 'wall', 'still', 'adsorb', 'nine', 'kill', 'host', 'five', 'multiply', 'naked', 'bacteria', 'forming', 'plaque', 'lawn', 'individual', 'gene', 'mutation', 'similarly', 'pleiotropic', 'effect', 'strongly', 'dependent', 'upon', 'plating', 'medium', 'thus', 'gta', 'cause', 'loss', 'glucosylation', 'teichoic', 'acid', 'result', 'adsorption', 'site', 'phi', 'membrane', 'phie', 'unstabilized', 'lysogenized', 'wild', 'type', 'also', 'exhibit', 'pattern', 'showing', 'different', 'level', 'resistance', 'similar', 'basis', 'bacterial', 'could', 'ordered', 'four', 'class', 'classified', 'six', 'group', 'together', 'complex', 'based', 'partly', 'perhaps', 'metabolic', 'block', 'development']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 3\n",
      "['localization', 'dehydrogenase', 'membrane', 'vesicle', 'prepared', 'escherichia', 'coli', 'wa', 'studied', 'using', 'antibody', 'purified', 'enzyme', 'activity', 'oxygen', 'uptake', 'french', 'press', 'completely', 'inhibited', 'suggesting', 'localized', 'outside', 'previous', 'result', 'futai', 'NUM', 'strongly', 'indicate', 'inversion', 'treatment', 'whereas', 'proline', 'transport', 'insensitive', 'suggest', 'inside', 'amino', 'acid', 'essentially', 'confirms', 'short', 'kaback', 'kohn', 'however', 'unlike', 'observed', 'small', 'significant', 'portion', 'sensitive', 'shown', 'may', 'represent', 'inverted', 'preparation', 'ferricyanide', 'reductase', 'detected', 'spheroplasts', 'total', 'ethylenediaminetetraacetate', 'finding', 'slightly', 'different', 'procedure', 'concluded', 'half', 'reactive', 'site', 'moved', 'remains']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 4\n",
      "['apparent', 'permeability', 'mutation', 'produced', 'escherichia', 'coli', 'bacteriophage', 'mutagenesis', 'pleiotropic', 'showing', 'sensitivity', 'number', 'detergent', 'unrelated', 'antibiotic', 'presumably', 'affect', 'cell', 'wall', 'membrane', 'biosynthesis', 'one', 'wa', 'genetically', 'mapped', 'site', 'near', 'acra', 'mtc', 'locus', 'approximately', 'NUM', 'min', 'taylor', 'trotter', 'map']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 5\n",
      "['formation', 'membrane', 'potential', 'energized', 'coli', 'cell', 'ha', 'investigated', 'mean', 'ionic', 'penetrants', 'flux', 'anion', 'cation', 'opposite', 'direction', 'observed', 'moved', 'uptake', 'wa', 'stoichiometrically', 'coupled', 'outflow', 'ion', 'value', 'calculated', 'distribution', 'permanent', 'range', 'mv', 'inside', 'minus', 'penetrating', 'deenergized', 'following', 'generation', 'influx', 'synthetic', 'natural', 'lactose', 'collapsed', 'effect', 'sensitive', 'maleimide', 'result', 'favour', 'conception', 'generates', 'driving', 'force', 'transport']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 6\n",
      "['propranolol', 'able', 'increase', 'amount', 'titratable', 'group', 'mitochondrial', 'membrane', 'effect', 'occurs', 'sonicated', 'particle', 'liposome', 'phenomenon', 'seen', 'presence', 'salt', 'solution', 'sucrose', 'fluorescence', 'sulphonate', 'suspension', 'counteracted', 'increasing', 'concentration', 'potassium', 'chloride', 'suggested', 'result', 'decrease', 'aggregation', 'phospholipid', 'time', 'environment', 'bound', 'molecule', 'hydrophobic', 'buffering', 'hydrophilicity', 'direct', 'inverse', 'correlation']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 7\n",
      "['pyruvate', 'oxidase', 'system', 'escherichia', 'coli', 'composed', 'soluble', 'flavoprotein', 'ec', 'NUM', 'ferricytochrome', 'oxidoreductase', 'electron', 'transport', 'associated', 'cell', 'fraction', 'membrane', 'particle', 'contain', 'lipid', 'weight', 'fractionation', 'revealed', 'abut', 'neutral', 'phospholipid', 'relative', 'ratio', 'ubiquinone', 'menaquinone', 'within', 'molar', 'basis', 'removal', 'extraction', 'aqueous', 'acetone', 'hydrolysis', 'treatment', 'bacillus', 'cereus', 'phospholipase', 'c', 'result', 'complete', 'loss', 'activity', 'analysis', 'extracted', 'practically', 'removed', 'phosphorus', 'however', 'diglycerides', 'produced', 'remain', 'addition', 'detergent', 'dodecylamide', 'material', 'restoration', 'original', 'completely', 'restores', 'furthermore', 'either', 'yeast', 'restore', 'quinone', 'supplemented', 'photoinactivated', 'noted', 'upon', 'reconstitution', 'fact', 'compound', 'appear', 'suppress', 'added', 'reaction', 'mixture', 'containing']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 8\n",
      "['glycerophospholipids', 'culture', 'neurospora', 'crassa', 'extracted', 'deacylated', 'analyzed', 'addition', 'strain', 'several', 'auxotrophic', 'mutant', 'examined', 'defective', 'phosphatidylethanolamine', 'methyltransferase', 'methionine', 'phosphatidylmonomethylethanolamine', 'dimethylethanolamine', 'inos', 'phosphatase', 'double', 'wa', 'constructed', 'grown', 'concentration', 'supplement', 'adequate', 'support', 'growth', 'bizarre', 'phospholipid', 'composition', 'appropriate', 'choice', 'possible', 'vary', 'relative', 'level', 'every', 'organism', 'exception', 'cardiolipin', 'maximum', 'range', 'encountered', 'zwitterionic', 'specie', 'expressed', 'per', 'cent', 'total', 'phosphorus', 'lecithin', 'NUM', 'phosphatidyldimethylethanolamine', 'anionic', 'phosphatidylserine', 'phosphatidylinositol', 'despite', 'wide', 'variation', 'proportion', 'individual', 'five', 'quantity', 'remained', 'constant', 'content', 'ratio', 'data', 'suggest', 'existence', 'internal', 'compensation', 'mechanism', 'net', 'effect', 'maintenance', 'fairly', 'contribution', 'component', 'membrane', 'charge']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 9\n",
      "['binding', 'degradation', 'gtp', 'guanosine', 'beta', 'triphosphate', 'gpp', 'nh', 'p', 'plasma', 'membrane', 'rat', 'liver', 'fat', 'cell', 'investigated', 'hydrolyzed', 'predominantly', 'nucleotide', 'pyrophosphohydrolases', 'whereas', 'primarily', 'phosphohydrolases', 'enzyme', 'specific', 'guanine', 'since', 'analogous', 'adenine', 'spare', 'hydrolysis', 'taken', 'site', 'extent', 'high', 'concentration', 'corresponding', 'fail', 'inhibit', 'uptake', 'appear', 'remains', 'essentially', 'intact', 'irrespective', 'degree', 'unbound', 'indicating', 'siteis', 'incapable', 'degrading', 'gdp', 'competitively', 'constant', 'three', 'similar', 'NUM', 'mum', 'range', 'required', 'effect', 'adenylate', 'cyclase', 'activity', 'inhibited', 'sulfhydryl', 'agent', 'suggesting', 'group', 'involved', 'process', 'contrast', 'accompanied', 'substantial', 'incubation', 'condition', 'atp', 'plus', 'regenerating', 'system', 'medium', 'bound', 'progressively', 'thus', 'although', 'differentially', 'susceptible', 'terminal', 'phosphate', 'finding', 'discussed', 'term', 'markedly', 'different', 'potency', 'activator']\n",
      "\n",
      "Synset('membrane.n.01')\n",
      "\n",
      "Test 10\n",
      "['escherichia', 'coli', 'salmonella', 'typhimurium', 'cell', 'wall', 'contains', 'outer', 'membrane', 'layer', 'peptidoglycan', 'act', 'barrier', 'molecular', 'sieve', 'type', 'penetration', 'uncharged', 'saccharide', 'decad', 'nakae', 'nikaido', 'NUM', 'fed', 'proc', 'examined', 'limited', 'size', 'penetrating', 'molecule', 'studying', 'whose', 'destroyed', 'lysozyme', 'treatment', 'growth', 'presence', 'penicillin', 'b', 'isolated', 'vesicle', 'found', 'similar', 'intact', 'plasmolyzed', 'allowed', 'partial', 'stachyose', 'weight', 'essentially', 'excluded', 'higher', 'also', 'acted', 'observation', 'led', 'u', 'conclude', 'rather', 'set', 'limit', 'hydrophilic', 'however', 'exclusion', 'much', 'leakiness', 'could', 'decreased', 'either', 'use', 'mutant', 'producing', 'extremely', 'deficient', 'lipopolysaccharide', 'trypsin', 'followed', 'heating', 'slow', 'cooling', 'feel', 'consistent', 'hypothesis', 'resealing', 'ruptured', 'isolation', 'procedure', 'often', 'incomplete', 'crack', 'hole', 'thus', 'generated', 'responsible']\n",
      "\n",
      "Synset('membrane.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Code to print out results and definitions of the first word #\n",
    "word1 = wn.synsets(words_2_senses[0][0])\n",
    "print(words_2_senses[0][0])\n",
    "print(word1)\n",
    "print(\"1st definition: {}\".format(word1[0].definition()))\n",
    "print(\"2nd definition: {}\".format(word1[1].definition()))\n",
    "print(\"Length of training data: {}\".format(length_of_training_data[0]))\n",
    "i = 0\n",
    "for test in evalulating_tests[0]:\n",
    "    i += 1\n",
    "    print(\"\")\n",
    "    print(\"Test {}\".format(i))\n",
    "    print(test[0])\n",
    "    print(\"\")\n",
    "    print(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membrane\n",
    "\n",
    "As seen in the printed statements above, 'membrane' has two definitions. In the Medline document the definition should be: \n",
    "> \"a pliable sheet of tissue that covers or lines or connects the organs or cells of animals or plants\"\n",
    "\n",
    "Unfortunately, none of the documents tested produced this definition. This is because the training reference document in the Semcor collection has the word 'membrane' classified as the other definition:\n",
    ">\"a thin pliable sheet of material\"\n",
    "\n",
    "The above is clearly not the definition of the first test sentence as this sentence includes the words:\n",
    "> 'multiplication', 'mumps', 'virus',  'cell', 'living', 'influenza',  and 'disease'\n",
    "\n",
    "By observation, the above words refer to the second definition. For comparision, here is another selection of words from the 4th test shown above which should be classified as the 2nd definition:\n",
    "> 'mutation', 'coli', 'bacteriophage', 'mutagenesis', 'antibiotic',  'cell', 'wall', 'biosynthesis', and 'genetically'\n",
    "\n",
    "In summary, the membrane classifier failed because it relied on only 1 document for training data. The only context it had for knowning which sense to classify the sentenence, was of the first sense, and it subsequently did this for every sentence here. Had the sentence been derived from the Brown corpus and had been classsed as the 2nd definition, we could technically say that the classifier was very accurate. This would result in a false assumption as it would be classifying the sentences with the only sense it knows. In conclusion the membrane classifier is not accurate. \n",
    "\n",
    "Furthermore, this shows a weakness in the WSD system, as the training data is not large enough to get an accurate classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molecular\n",
      "[Synset('molecular.a.01'), Synset('molecular.a.02')]\n",
      "1st definition: relating to or produced by or consisting of molecules\n",
      "2nd definition: relating to simple or elementary organization; --G.A. Miller\n",
      "Length of training data: 6\n",
      "\n",
      "Test 1\n",
      "['three', 'enzyme', 'one', 'ec', 'NUM', 'peptide', 'hydrolase', 'activity', 'separated', 'partially', 'purified', 'bacillus', 'subtilis', 'distinguished', 'respect', 'molecular', 'weight', 'catalytic', 'property', 'studied', 'relation', 'physiology', 'bacterium', 'designated', 'aminopeptidase', 'ha', 'produced', 'early', 'growth', 'hydrolyzes', 'rapidly', 'another', 'ii', 'also', 'third', 'iii', 'predominantly', 'stationary', 'phase', 'efficiently', 'utilizes', 'substrate', 'synthesis', 'suggests', 'selective', 'catabolism', 'occurs', 'time', 'perhaps', 'related', 'cessation', 'onset', 'event', 'dipeptide', 'well', 'identified', 'localized', 'cell', 'wall', 'periplasm', 'organism', 'evidence', 'variation', 'cycle', 'suggest', 'important', 'function', 'antibiotic', 'metabolism']\n",
      "\n",
      "Synset('molecular.a.01')\n",
      "\n",
      "Test 2\n",
      "['two', 'eighteen', 'strain', 'bacillus', 'subtilis', 'examined', 'contained', 'covalently', 'closed', 'circular', 'duplex', 'deoxyribonucleic', 'acid', 'dna', 'homogeneous', 'size', 'buoyant', 'density', 'atcc', 'NUM', 'copy', 'per', 'chromosome', 'plasmid', 'element', 'molecular', 'weight', 'time', 'one', 'appeared', 'closely', 'related', 'genetic', 'physiological', 'biochemical', 'criterion', 'much', 'le', 'pumilus', 'class', 'respectively', 'present', 'several']\n",
      "\n",
      "Synset('molecular.a.01')\n",
      "\n",
      "Test 3\n",
      "['two', 'clinical', 'isolates', 'bacteroides', 'contained', 'covalently', 'closed', 'circular', 'deoxyribonucleic', 'acid', 'dna', 'shown', 'sedimentation', 'alkaline', 'sucrose', 'gradient', 'cscl', 'ethidium', 'bromide', 'equilibrium', 'centrifugation', 'electron', 'microscopy', 'bacteriodes', 'fragilis', 'homogeneous', 'specie', 'plasmid', 'molecular', 'weight', 'NUM', 'x', 'ochraceus', 'distinct', 'element', 'larger', 'cosedimented', 'form', 'r', 'corresponding', 'smaller', 'sedimented', 'molecule', 'calculated', 'role', 'unknown', 'neither', 'strain', 'transferred', 'antibiotic', 'resistance', 'escherichia', 'coli', 'produced', 'bacteriocins', 'active', 'sensitive', 'indicator']\n",
      "\n",
      "Synset('molecular.a.01')\n",
      "\n",
      "Test 4\n",
      "['human', 'liver', 'ha', 'purified', 'apparent', 'homogeneity', 'NUM', 'yield', 'affinity', 'chromatographic', 'procedure', 'utilizing', 'agarose', 'isoelectric', 'focusing', 'revealed', 'six', 'form', 'enzyme', 'polyacrylamide', 'gel', 'electrophoresis', 'demonstrated', 'presence', 'band', 'protein', 'contained', 'fucosidase', 'activity', 'preparation', 'wa', 'found', 'contain', 'trace', 'amount', 'seven', 'glycosidases', 'quantitative', 'amino', 'acid', 'analysis', 'performed', 'preliminary', 'carbohydrate', 'indicated', 'molecule', 'filtration', 'sepharose', 'approximate', 'molecular', 'weight', 'high', 'speed', 'sedimentation', 'equilibrium', 'yielded', 'sodium', 'dodecyl', 'sulfate', 'single', 'subunit', 'ph', 'optimum', 'suggested', 'second', 'michaelis', 'constant', 'maximal', 'velocity', 'determined', 'respect', 'substrate', 'mm', 'respectively', 'several', 'salt', 'little', 'effect', 'although', 'completely', 'inactivated', 'antibody', 'made', 'dound', 'monospecific', 'crude', 'supernatant', 'fluid', 'pure', 'antigen', 'material', 'detected', 'patient', 'died', 'fucosidosis']\n",
      "\n",
      "Synset('molecular.a.01')\n",
      "\n",
      "Test 5\n",
      "['three', 'fragment', 'specie', 'isolated', 'time', 'plasmin', 'digest', 'fibrinogen', 'molecular', 'weight', 'summation', 'subunit', 'sodium', 'dodecyl', 'sulfate', 'polyacrylamide', 'gel', 'electrophoresis', 'sedimentation', 'equilibrium', 'ultracentrifugation', 'NUM', 'depending', 'value', 'calculated', 'partial', 'specific', 'volume', 'contained', 'derived', 'aalpha', 'bbeta', 'gamma', 'chain', 'differed', 'extent', 'degradation', 'derivative', 'cleaved', 'release', 'site', 'however', 'beta', 'alpha', 'resistant', 'digestion', 'highly', 'fibrin', 'dimeric', 'structure', 'due', 'formation', 'two', 'wa', 'well', 'incorporating', 'fluorescent', 'label', 'monodansyl', 'cadaverine', 'acceptor', 'prevented', 'therefore', 'one', 'monomer', 'generated', 'result', 'show', 'unequivocally', 'contains', 'must', 'production', 'molecule', 'recently', 'proposed', 'model', 'cleavage', 'postulate', 'generation', 'single', 'pair', 'incorrect', 'incorporation', 'alter', 'detectably', 'series', 'peptide', 'ranged', 'early', 'stage', 'degraded', 'progressively', 'brightly', 'weakly', 'thus', 'within', 'segment']\n",
      "\n",
      "Synset('molecular.a.01')\n",
      "\n",
      "Test 6\n",
      "['thyroglobulin', 'obtained', 'guinea', 'pig', 'wa', 'examined', 'na', 'gel', 'electrophoresis', 'reduction', 'alkylation', 'contrast', 'mammalian', 'source', 'three', 'group', 'polypeptide', 'chain', 'accounted', 'NUM', 'protein', 'determination', 'molecular', 'weight', 'purified', 'equilibrium', 'centrifugation', 'guanidine', 'hcl', 'gave', 'value', 'specie', 'b', 'c', 'determined', 'filtration', 'similar', 'result', 'due', 'large', 'size', 'satisfactory', 'could', 'amino', 'acid', 'analysis', 'whole', 'slightly', 'higher', 'level', 'lysine', 'histidine', 'lower', 'glutamic', 'seen', 'iodine', 'content', 'found', 'range', 'respectively']\n",
      "\n",
      "Synset('molecular.a.01')\n",
      "\n",
      "Test 7\n",
      "['escherichia', 'coli', 'salmonella', 'typhimurium', 'cell', 'wall', 'contains', 'outer', 'membrane', 'layer', 'peptidoglycan', 'act', 'barrier', 'molecular', 'sieve', 'type', 'penetration', 'uncharged', 'saccharide', 'decad', 'nakae', 'nikaido', 'NUM', 'fed', 'proc', 'examined', 'limited', 'size', 'penetrating', 'molecule', 'studying', 'whose', 'destroyed', 'lysozyme', 'treatment', 'growth', 'presence', 'penicillin', 'b', 'isolated', 'vesicle', 'found', 'similar', 'intact', 'plasmolyzed', 'allowed', 'partial', 'stachyose', 'weight', 'essentially', 'excluded', 'higher', 'also', 'acted', 'observation', 'led', 'u', 'conclude', 'rather', 'set', 'limit', 'hydrophilic', 'however', 'exclusion', 'much', 'leakiness', 'could', 'decreased', 'either', 'use', 'mutant', 'producing', 'extremely', 'deficient', 'lipopolysaccharide', 'trypsin', 'followed', 'heating', 'slow', 'cooling', 'feel', 'consistent', 'hypothesis', 'resealing', 'ruptured', 'isolation', 'procedure', 'often', 'incomplete', 'crack', 'hole', 'thus', 'generated', 'responsible']\n",
      "\n",
      "Synset('molecular.a.01')\n"
     ]
    }
   ],
   "source": [
    "# Code to print out results and definitions of the second word # \n",
    "word2 = wn.synsets(words_2_senses[1][0])\n",
    "print(words_2_senses[1][0])\n",
    "print(word2)\n",
    "print(\"1st definition: {}\".format(word2[0].definition()))\n",
    "print(\"2nd definition: {}\".format(word2[1].definition()))\n",
    "print(\"Length of training data: {}\".format(length_of_training_data[1]))\n",
    "i = 0\n",
    "for test in evalulating_tests[1]:\n",
    "    i += 1\n",
    "    print(\"\")\n",
    "    print(\"Test {}\".format(i))\n",
    "    print(test[0])\n",
    "    print(\"\")\n",
    "    print(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular\n",
    "As seen above 'Molecular' has two definitions. The 1st of which is relating to the Medline corpus:\n",
    "> \"relating to or produced by or consisting of molecules\"\n",
    "\n",
    "This refers to the molecules in elements and the body, and therefore all documents should in theory be referring to this definition. The other definition which isn't related to the Medline corpus:\n",
    ">\"relating to simple or elementary organization; --G.A. Miller\"\n",
    "\n",
    "The above is very similar in description in my opinion, which doesn't help as this could confuse the classifier. However, the classifier has classed all Molecular documents as the first definition, which results in success. However, that success is limited as the Molecular classifier was only trained on 6 documents, all of which could be the first definition. The classifier appears correct though as we see in the first test:\n",
    ">'activity', 'purified', 'weight', 'catalytic', 'property', 'bacterium', 'produced', 'early', 'growth', 'predominantly', 'synthesis',  'cell', 'organism', 'cycle', 'antibiotic',  and 'metabolism'\n",
    "\n",
    "These words selected from the first document show that molecular in this document was referring to the 1st definition. The word \"weight\" seems to appear after our key word Molecular in some of the documents, such as Test 2 and 6. This shows that the classifier identifies this and knows this refers to the 1st definition of Molecular. \n",
    "\n",
    "In Summary, the molecular classifier is accurate but should have more test documents in order to make sure that it does understand each sense of molecular. This shows a strength to the WSD algorithm as it is correct when trained on the right sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature\n",
      "[Synset('temperature.n.01'), Synset('temperature.n.02')]\n",
      "1st definition: the degree of hotness or coldness of a body or environment (corresponding to its molecular activity)\n",
      "2nd definition: the somatic sensation of cold or heat\n",
      "Length of training data: 67\n",
      "\n",
      "Test 1\n",
      "['effect', 'elevated', 'temperature', 'growth', 'rate', 'wa', 'studied', 'five', 'strain', 'enterobacteriaceae', 'tested', 'shift', 'resulted', 'immediate', 'decrease', 'due', 'limitation', 'availability', 'endogenous', 'methionine', 'first', 'biosynthetic', 'enzyme', 'extract', 'aerobacter', 'aerogenes', 'salmonella', 'typhimurium', 'escherichia', 'coli', 'shown', 'sensitive']\n",
      "\n",
      "Synset('temperature.n.02')\n",
      "\n",
      "Test 2\n",
      "['localization', 'mutation', 'affecting', 'ribonuclease', 'iii', 'activity', 'enzyme', 'specific', 'ribonucleic', 'acid', 'escherichia', 'coli', 'wa', 'attempted', 'series', 'mating', 'transduction', 'experiment', 'mapped', 'near', 'nadb', 'gene', 'strain', 'carrying', 'another', 'also', 'found', 'based', 'available', 'data', 'order', 'chromosome', 'appears', 'tyra', 'rana', 'rnc', 'puri', 'either', 'fail', 'grow', 'NUM', 'c', 'enriched', 'medium', 'whereas', 'defective', 'slowly', 'corresponding', 'tested', 'temperature', 'reduces', 'growth', 'rate', 'bacteriophage', 'form', 'plaque', 'lower', 'efficiency', 'thus', 'suggest', 'beneficial', 'normal', 'higher', 'becomes', 'indispensable']\n",
      "\n",
      "Synset('temperature.n.02')\n",
      "\n",
      "Test 3\n",
      "['mutant', 'escherichia', 'coli', 'conditionally', 'lethal', 'mutation', 'structural', 'gene', 'deoxyribonucleic', 'acid', 'dna', 'ligase', 'extensive', 'repair', 'synthesis', 'occurred', 'cell', 'nonpermissive', 'temperature', 'NUM', 'permissive', 'c', 'nearly', 'normal', 'semiconservative', 'limited', 'observed', 'wa', 'activated', 'addition', 'nicotinamide', 'adenine', 'dinucleotide']\n",
      "\n",
      "Synset('temperature.n.02')\n",
      "\n",
      "Test 4\n",
      "['NUM', 'primary', 'culture', 'sarcoma', 'cell', 'exhibit', 'much', 'greater', 'activity', 'normal', 'connective', 'tissue', 'grown', 'adult', 'blood', 'vessel', 'shorter', 'latent', 'period', 'ameboid', 'phenomenon', 'marked', 'multiplication', 'proceeds', 'rapidly', 'secondary', 'le', 'active', 'hand', 'show', 'markedly', 'accelerated', 'growth', 'easily', 'propagated', 'long', 'vitro', 'multiply', 'actively', 'three', 'month', 'old', 'method', 'cultivation', 'well', 'adapted', 'study', 'pathological', 'division', 'nuclear', 'change', 'discernible', 'living', 'staining', 'may', 'applied', 'verify', 'observation', 'upon', 'unstained', 'structure', 'atypical', 'mitoses', 'several', 'kind', 'found', 'seen', 'time', 'required', 'rat', 'kept', 'body', 'temperature', 'degree', 'varies', 'within', 'relatively', 'narrow', 'limit', 'twenty', 'fifty', 'minute', 'contrary', 'variation', 'hour', 'amitotic', 'ha', 'observed', 'either', 'tumor', 'evidence', 'budding', 'however', 'formation', 'containing', 'nucleus', 'irregular', 'size', 'noted', 'development', 'two', 'mononuclear', 'mitotic', 'without', 'cytoplasm', 'also']\n",
      "\n",
      "Synset('temperature.n.02')\n",
      "\n",
      "Test 5\n",
      "['precise', 'oxygen', 'equilibrium', 'curve', 'human', 'adult', 'hemoglobin', 'determined', 'automatic', 'recording', 'method', 'several', 'temperature', 'presence', 'absence', 'dpg', 'inositol', 'hexaphosphate', 'ihp', 'NUM', 'hydroxymethyl', 'buffer', 'ph', 'containing', 'data', 'analyzed', 'according', 'adair', 'scheme', 'heat', 'deltahi', 'entropy', 'change', 'deltasi', 'individual', 'oxygenation', 'step', 'obtained', 'shape', 'varies', 'whether', 'present', 'absent', 'consequence', 'value', 'depends', 'behavior', 'similar', 'resulted', 'compensation', 'phenomenon', 'contribution', 'cdeltasi', 'free', 'energy', 'compensated', 'first', 'three', 'fourth', 'upon', 'addition', 'accompanied', 'c', 'k', 'major', 'part', 'nonuniformity', 'appears', 'attributable', 'binding', 'result', 'necessarily', 'support', 'earlier', 'idea', 'wyman', 'cooperative', 'oxygenbinding', 'essentially', 'effect']\n",
      "\n",
      "Synset('temperature.n.02')\n",
      "\n",
      "Test 6\n",
      "['binding', 'site', 'located', 'atpase', 'purified', 'fragmented', 'sarcoplasmic', 'reticulum', 'ikemoto', 'n', 'NUM', 'biol', 'chem', 'studied', 'degree', 'three', 'class', 'denoted', 'alpha', 'k', 'congruent', 'time', 'beta', 'gamma', 'two', 'per', 'dalton', 'one', 'change', 'reversible', 'parallelism', 'reported', 'sumida', 'tonomura', 'biochem', 'temperature', 'dependence', 'ratio', 'transport', 'atp', 'cleavage', 'suggests', 'involvement', 'study', 'low', 'enzyme', 'mol', 'g', 'unit', 'permitting', 'separate', 'investigation', 'phosphorylation', 'dephosphorylation', 'process', 'show', 'concomitantly', 'formation', 'phosphorylated', 'e', 'approximately', 'p', 'bound', 'calcium', 'released', 'rebound', 'moiety', 'inhibits', 'liberation', 'pi', 'analysis', 'use', 'hill', 'plot', 'inhibition', 'average', 'affinity', 'tentatively', 'identified', 'form']\n",
      "\n",
      "Synset('temperature.n.02')\n",
      "\n",
      "Test 7\n",
      "['ionization', 'constant', 'NUM', 'histidine', 'residue', 'ribonuclease', 'beenobtained', 'temperature', 'nuclear', 'magnetic', 'resonance', 'titration', 'curve', 'imidazole', 'proton', 'thermodynamic', 'parameter', 'derived', 'indicate', 'fairly', 'well', 'exposed', 'solvent', 'somewhat', 'restricted', 'environment', 'measurement', 'low', 'ph', 'inflection', 'present', 'yield', 'large', 'negative', 'entropy', 'value', 'indicating', 'group', 'givine', 'rise', 'also', 'buried']\n",
      "\n",
      "Synset('temperature.n.02')\n"
     ]
    }
   ],
   "source": [
    "# Code to print out the results and definitions of the third word #\n",
    "word3 = wn.synsets(words_2_senses[2][0])\n",
    "print(words_2_senses[2][0])\n",
    "print(word3)\n",
    "print(\"1st definition: {}\".format(word3[0].definition()))\n",
    "print(\"2nd definition: {}\".format(word3[1].definition()))\n",
    "print(\"Length of training data: {}\".format(length_of_training_data[2]))\n",
    "i = 0\n",
    "for test in evalulating_tests[2]:\n",
    "    i += 1\n",
    "    print(\"\")\n",
    "    print(\"Test {}\".format(i))\n",
    "    print(test[0])\n",
    "    print(\"\")\n",
    "    print(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature\n",
    "\n",
    "As seen above Temperature has two definitions, the 2nd definition relates to the human body temperature, which is what we can assume the documents are referring to:\n",
    "\n",
    ">the somatic sensation of cold or heat\n",
    "\n",
    "The first definition also refers to a body of sorts however, and therefore the temperature sense should be in theory be difficult to determine:\n",
    "\n",
    ">the degree of hotness or coldness of a body or environment (corresponding to its molecular activity)\n",
    "\n",
    "This is mostly due to the brackets that feature the word \"molecular\", but in general the 2nd definition is what we are looking for. The classification of the 1st document is correct therefore as it features the words:\n",
    "> 'effect', 'elevated',  'growth', 'rate', 'enterobacteriaceae', 'biosynthetic', 'enzyme', 'extract',  'salmonella',  and 'coli'\n",
    "\n",
    "These words relate to the body and hence the second definition. However, the 7th document's classification is wrong due to it including the words:\n",
    ">'ionization', 'constant',  'nuclear', 'magnetic',  'proton', 'thermodynamic', 'restricted', 'environment', 'measurement', 'low', 'ph', and 'inflection'\n",
    "\n",
    "The 7th document is talking about an experiment into the 1st sense of temperature, but unfortunately this was classified as the 2nd sense. This shows a weakness in the WSD system as it does not produce accurate results even with more extensive test data. With a difference between both senses, it picks one sense and classifies all with that sense. However, provided the WSD system knows about the correct sense for the Medline corpus then it appears to answer correctly, this isn't how a classifier should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the best strength of this WSD system is the speed at which it runs, due to the use of functions and imports, which themseleves are highly efficient. However based on the current setup, it does not efficiently classify the sense of the word given a sentence as context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How could you extend or improve your WSD system?  You are **not** expected to code any of these extensions or improvements, but your answer should give sufficient details to make it clear how they might be carried out in practice. \\[5 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ways to Extend the WSD system: \n",
    "* Include all the words from the 2 sense top 10 list, this would allow a greater scope of the words in the Medline corpus and their most likely senses, such as:\n",
    "> p, iii, mph, uptake, may, amino, molecule\n",
    "\n",
    "* Use of more than one type of human sense annotated training corpus to train the classifier so that it gets a wide scope of definitions, and potentially more training data that has different senses of the given word.\n",
    "\n",
    "##### Ways to Improve the WSD system: \n",
    "* Use of a human sense annotated Medline corpus as this would give a more accurate training sense for the classifier \n",
    "\n",
    "* The use of the Brown corpus on this algorithm instead, as the top 10 2 sense words would be represented more by the same corpus as the corpus for testing \n",
    "\n",
    "* The use of words that aren't always associated with medical situations would also help, as these words may be more represented in the Brown corpus. Such as:\n",
    "> mph, iii, uptake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to verify that the length of your submission does not exceed 2000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission length is 1992\n"
     ]
    }
   ],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "##Running it before providing any answers shows that the questions have a word count of 1202\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "\n",
    "filepath=\"a2.ipynb\"\n",
    "question_count=626\n",
    "\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
